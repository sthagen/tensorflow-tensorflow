
--- a/include/triton/Conversion/MLIRTypes.h	2024-07-03 07:14:55.000000000 -0700
+++ b/include/triton/Conversion/MLIRTypes.h	2025-01-19 13:19:21.000000000 -0800
@@ -21,10 +21,10 @@
 }
 
 // Float types
-inline Type f16Ty(MLIRContext *ctx) { return FloatType::getF16(ctx); }
-inline Type f32Ty(MLIRContext *ctx) { return FloatType::getF32(ctx); }
-inline Type f64Ty(MLIRContext *ctx) { return FloatType::getF64(ctx); }
-inline Type bf16Ty(MLIRContext *ctx) { return FloatType::getBF16(ctx); }
+inline Type f16Ty(MLIRContext *ctx) { return Float16Type::get(ctx); }
+inline Type f32Ty(MLIRContext *ctx) { return Float32Type::get(ctx); }
+inline Type f64Ty(MLIRContext *ctx) { return Float64Type::get(ctx); }
+inline Type bf16Ty(MLIRContext *ctx) { return BFloat16Type::get(ctx); }
 
 inline bool isFloat(Type type) {
   return type.isF32() || type.isF64() || type.isF16() || type.isF128() ||

--- a/lib/Dialect/TritonGPU/IR/Ops.cpp	2025-01-15 12:52:52.000000000 -0800
+++ b/lib/Dialect/TritonGPU/IR/Ops.cpp	2025-01-19 13:19:21.000000000 -0800
@@ -15,7 +15,7 @@
   auto xTy = getSrc().getType();
   auto scaleTy = getScale().getType();
 
-  if (xTy.getElementType() != FloatType::getBF16(getContext()) &&
+  if (xTy.getElementType() != BFloat16Type::get(getContext()) &&
       xTy.getElementType() != IntegerType::get(getContext(), 8)) {
     return emitOpError("element type of the first operand must be bf16 or i8");
   }
@@ -111,7 +111,7 @@
     auto newShape = SmallVector<int64_t>(xShape);
     if (!encoding) {
       newShape.back() *= 2;
-      retTy = RankedTensorType::get(xShape, FloatType::getBF16(ctx));
+      retTy = RankedTensorType::get(xShape, BFloat16Type::get(ctx));
     } else {
       auto oldEncoding = cast<DotOperandEncodingAttr>(encoding);
       auto newVEncoding = DotOperandEncodingAttr::get(
@@ -123,7 +123,7 @@
       const bool hasBatch = xShape.size() == 3;
       const int kIdx = (opIdx == 0 ? 1 : 0) + hasBatch;
       newShape[kIdx] *= 2;
-      retTy = RankedTensorType::get(newShape, FloatType::getBF16(ctx),
+      retTy = RankedTensorType::get(newShape, BFloat16Type::get(ctx),
                                     newVEncoding);
     }
     inferredReturnTypes.push_back(retTy);

--- a/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp	2025-01-15 12:52:52.000000000 -0800
+++ b/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp	2025-01-19 13:19:22.000000000 -0800
@@ -56,9 +56,9 @@
   else if (constraint == 'l')
     ty = IntegerType::get(rewriter.getContext(), 64);
   else if (constraint == 'f')
-    ty = FloatType::getF32(rewriter.getContext());
+    ty = Float32Type::get(rewriter.getContext());
   else if (constraint == 'd')
-    ty = FloatType::getF64(rewriter.getContext());
+    ty = Float64Type::get(rewriter.getContext());
   else {
     assert(false && "Unsupported constraint");
   }

--- a/unittest/Dialect/TritonGPU/DialectTest.cpp	2025-01-15 12:52:52.000000000 -0800
+++ b/unittest/Dialect/TritonGPU/DialectTest.cpp	2025-01-19 13:19:23.000000000 -0800
@@ -492,10 +492,10 @@
         llvm::to_vector(llvm::reverse(llvm::seq<unsigned>(rank))));
 
     auto srcTy = RankedTensorType::get(
-        srcShape, FloatType::getF32(&ctx),
+        srcShape, Float32Type::get(&ctx),
         BlockedEncodingAttr::get(&ctx, sizePerThread, threadsPerWarp,
                                  warpsPerCTA, order, ctaLayout));
-    auto dstTy = RankedTensorType::get(dstShape, FloatType::getF32(&ctx));
+    auto dstTy = RankedTensorType::get(dstShape, Float32Type::get(&ctx));
 
     bool couldReshape = false;
     testReshape(srcTy, dstTy, /*expectedDstEnc=*/std::nullopt,
@@ -526,7 +526,7 @@
     ctx.getOrLoadDialect<TritonGPUDialect>();
     ctaLayout =
         triton::gpu::CTALayoutAttr::get(&ctx, ctaPerCGA, ctaSplit, ctaOrder);
-    f16Ty = FloatType::getF16(&ctx);
+    f16Ty = Float16Type::get(&ctx);
   }
 
   triton::gpu::AMDMfmaEncodingAttr createMFMA(int mDim, int nDim,
@@ -692,7 +692,7 @@
       ASSERT_EQ(linearLayout, expandedLL);
 
       // Test that methods of DistributedEncoding return the same values
-      Type eltTy = FloatType::getF32(&ctx);
+      Type eltTy = Float32Type::get(&ctx);
 
       ASSERT_EQ(getOrder(distributedEncoding), linearEncoding.getRepOrder());
       ASSERT_EQ(cast<triton::gpu::TritonGPU_AttrTrait>(distributedEncoding)

--- a/unittest/Dialect/TritonGPU/DumpLayoutTest.cpp	2024-10-31 04:36:20.000000000 -0700
+++ b/unittest/Dialect/TritonGPU/DumpLayoutTest.cpp	2025-01-19 13:19:23.000000000 -0800
@@ -182,7 +182,7 @@
                              {1},   /* ord, row-major */
                              {1});  /* cOrd */
 
-  auto elemTy = FloatType::getF16(sharedLayout.getContext());
+  auto elemTy = Float16Type::get(sharedLayout.getContext());
   auto tensorType = RankedTensorType::get({32}, elemTy, sharedLayout);
   std::string layout = getLayoutStr(tensorType, /*useHWPointOfView=*/false);
   assertSameStr(refStr, layout);
@@ -237,7 +237,7 @@
                              {1, 0},  /* ord, row-major */
                              {1, 0}); /* cOrd */
 
-  auto elemTy = FloatType::getF16(sharedLayout.getContext());
+  auto elemTy = Float16Type::get(sharedLayout.getContext());
   auto tensorType = RankedTensorType::get({8, 32}, elemTy, sharedLayout);
   std::string layout = getLayoutStr(tensorType, /*useHWPointOfView=*/false);
   assertSameStr(refStr, layout);
@@ -510,7 +510,7 @@
                                {1, 0},  /* ord, row-major */
                                {1, 0}); /* cOrd */
 
-  auto elemTyHW = FloatType::getF16(sharedLayoutHW.getContext());
+  auto elemTyHW = Float16Type::get(sharedLayoutHW.getContext());
   auto tensorTypeHW = RankedTensorType::get({8, 32}, elemTyHW, sharedLayoutHW);
 
   std::string layoutHW = getLayoutStr(tensorTypeHW, /*useHWPointOfView=*/true);
