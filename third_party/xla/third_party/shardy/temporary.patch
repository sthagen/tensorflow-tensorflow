diff --git a/shardy/dialect/sdy/ir/canonicalization.cc b/shardy/dialect/sdy/ir/canonicalization.cc
index e1b391f..fb07fd1 100644
--- a/shardy/dialect/sdy/ir/canonicalization.cc
+++ b/shardy/dialect/sdy/ir/canonicalization.cc
@@ -117,6 +117,35 @@ class RedundantManualComputationPattern
   }
 };
 
+// Pattern to remove duplicate (same input and group id) ShardingGroupOps within
+// the same block.
+class DedupShardingGroupPattern : public OpRewritePattern<ShardingGroupOp> {
+ public:
+  using OpRewritePattern<ShardingGroupOp>::OpRewritePattern;
+  void initialize() { setDebugName("DedupShardingGroupPattern"); }
+
+ private:
+  LogicalResult matchAndRewrite(ShardingGroupOp shardingGroupOp,
+                                PatternRewriter& rewriter) const override {
+    // Loop over all ShardingGroupOps with the same input. IF there are any
+    // ShardingGroupOps with the same group id, THEN delete the current op.
+    bool anyDeduped = false;
+    for (Operation* otherOp :
+         llvm::make_early_inc_range(shardingGroupOp.getInput().getUsers())) {
+      if (otherOp == shardingGroupOp) {
+        continue;
+      }
+      if (auto otherShardingGroupOp = dyn_cast<ShardingGroupOp>(otherOp);
+          otherShardingGroupOp != nullptr &&
+          otherShardingGroupOp.getGroupId() == shardingGroupOp.getGroupId()) {
+        rewriter.eraseOp(otherOp);
+        anyDeduped = true;
+      }
+    }
+    return success(anyDeduped);
+  }
+};
+
 }  // namespace
 
 void ManualComputationOp::getCanonicalizationPatterns(
@@ -130,6 +159,11 @@ void ReshardOp::getCanonicalizationPatterns(RewritePatternSet& results,
   results.add<ReshardOfReshardPattern>(context);
 }
 
+void ShardingGroupOp::getCanonicalizationPatterns(RewritePatternSet& results,
+                                                  MLIRContext* context) {
+  results.add<DedupShardingGroupPattern>(context);
+}
+
 void AllGatherOp::getCanonicalizationPatterns(RewritePatternSet& results,
                                               MLIRContext* context) {
   results.add<AllGatherNoopPattern>(context);
diff --git a/shardy/dialect/sdy/ir/ops.td b/shardy/dialect/sdy/ir/ops.td
index 9891396..d4ef364 100644
--- a/shardy/dialect/sdy/ir/ops.td
+++ b/shardy/dialect/sdy/ir/ops.td
@@ -219,6 +219,7 @@ def Sdy_ShardingGroupOp : Sdy_Op<"sharding_group",
   let results = (outs);
 
   let assemblyFormat = "$input `group_id````=```$group_id attr-dict `:` type($input)";
+  let hasCanonicalizer = 1;
 }
 
 def Sdy_ConstantOp : Sdy_Op<"constant",
diff --git a/shardy/dialect/sdy/ir/test/sharding_group_canonicalization.mlir b/shardy/dialect/sdy/ir/test/sharding_group_canonicalization.mlir
new file mode 100644
index 0000000..a08911c
--- /dev/null
+++ b/shardy/dialect/sdy/ir/test/sharding_group_canonicalization.mlir
@@ -0,0 +1,62 @@
+// RUN: sdy_opt %s -canonicalize | FileCheck %s
+
+// CHECK-LABEL: func @duplicate_shard_group_ops
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=123
+// CHECK-NEXT: return
+func.func @duplicate_shard_group_ops(%arg0: tensor<8xf32>) {
+  sdy.sharding_group %arg0 group_id=123 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=123 : tensor<8xf32>
+  func.return
+}
+
+// CHECK-LABEL: func @duplicate_multiple_shard_group_ops
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=52
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=1117
+// CHECK-NEXT: return
+func.func @duplicate_multiple_shard_group_ops(%arg0: tensor<8xf32>) {
+  sdy.sharding_group %arg0 group_id=1117 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=52 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=1117 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=52 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=52 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=1117 : tensor<8xf32>
+  func.return
+}
+
+// CHECK-LABEL: func @duplicate_multiple_shard_group_ops_different_args
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=19
+// CHECK-DAG: sdy.sharding_group %arg1 group_id=19
+// CHECK-DAG: sdy.sharding_group %arg1 group_id=217
+// CHECK-NEXT: return
+func.func @duplicate_multiple_shard_group_ops_different_args(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) {
+  sdy.sharding_group %arg0 group_id=19 : tensor<8xf32>
+  sdy.sharding_group %arg1 group_id=19 : tensor<8xf32>
+  sdy.sharding_group %arg0 group_id=19 : tensor<8xf32>
+  sdy.sharding_group %arg1 group_id=217 : tensor<8xf32>
+  sdy.sharding_group %arg1 group_id=217 : tensor<8xf32>
+  sdy.sharding_group %arg1 group_id=19 : tensor<8xf32>
+  func.return
+}
+
+// CHECK-LABEL: func @shard_group_no_dup_with_other_op
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=19
+// CHECK-NEXT: %0 = stablehlo.add %arg0, %arg0
+// CHECK-NEXT: return
+func.func @shard_group_no_dup_with_other_op(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
+  sdy.sharding_group %arg0 group_id=19 : tensor<16x32xf32>
+  %0 = stablehlo.add %arg0, %arg0 : tensor<16x32xf32>
+  func.return %0 : tensor<16x32xf32>
+}
+
+// CHECK-LABEL: func @shard_group_dup_with_mult_other_op
+// CHECK-DAG: sdy.sharding_group %arg0 group_id=19
+// CHECK-NEXT: %0 = stablehlo.add %arg0, %arg0
+// CHECK-NEXT: %1 = stablehlo.add %arg0, %0
+// CHECK-NEXT: return
+func.func @shard_group_dup_with_mult_other_op(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
+  sdy.sharding_group %arg0 group_id=19 : tensor<16x32xf32>
+  %0 = stablehlo.add %arg0, %arg0 : tensor<16x32xf32>
+  sdy.sharding_group %arg0 group_id=19 : tensor<16x32xf32>
+  %1 = stablehlo.add %arg0, %0 : tensor<16x32xf32>
+  func.return %1 : tensor<16x32xf32>
+}
diff --git a/shardy/dialect/sdy/transforms/import/import_pipeline.cc b/shardy/dialect/sdy/transforms/import/import_pipeline.cc
index 941273c..4bf1f92 100644
--- a/shardy/dialect/sdy/transforms/import/import_pipeline.cc
+++ b/shardy/dialect/sdy/transforms/import/import_pipeline.cc
@@ -63,6 +63,10 @@ void addImportPipeline(OpPassManager& pm, StringRef dumpDirectory,
   // constraints. This ensures we can detect sharding conflicts between group
   // members which have pre-propagation shardings due to sharding constraints.
   pm.addPass(createShardingGroupImportPass());
+  pm.addPass(createCanonicalizerPass(
+      getCanonicalizerConfig(/*enableRegionSimplification=*/false),
+      /*disabledPatterns=*/{},
+      /*enabledPatterns=*/{"DedupShardingGroupPattern"}));
   pm.addPass(mlir::sdy::createSaveModuleOpPass(dumpDirectory,
                                                "sdy_module_after_sdy_import"));
 }
diff --git a/shardy/dialect/sdy/transforms/import/sharding_group_import.cc b/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
index 6cfed8f..74d66fe 100644
--- a/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
+++ b/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
@@ -15,18 +15,12 @@ limitations under the License.
 
 #include <cstdint>
 #include <memory>  // IWYU pragma: keep
-#include <utility>
 
 #include "llvm/ADT/DenseMap.h"
-#include "llvm/ADT/DenseSet.h"
-#include "llvm/ADT/EquivalenceClasses.h"
 #include "llvm/ADT/MapVector.h"
-#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/EquivalenceClasses.h"
 #include "llvm/ADT/SmallVector.h"
-#include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/PatternMatch.h"
-#include "mlir/IR/SymbolTable.h"
 #include "mlir/IR/Value.h"
 #include "mlir/IR/Visitors.h"
 #include "mlir/Pass/Pass.h"  // IWYU pragma: keep
@@ -134,101 +128,43 @@ GroupIdToShardingGroups unifyShardingGroups(
   return reindexGroups;
 }
 
-// Erase any duplicate `ShardingGroupOp`s operating on the same input value.
-// TODO(bartchr): Also consider removing sharding groups of size 1. This would
-// require re-normalizing the group ids.
-void eraseDuplicateGroups(GroupIdToShardingGroups& groupIdToShardingGroups) {
-  for (SmallVector<ShardingGroupOp>& shardingGroupOps :
-       groupIdToShardingGroups) {
-    llvm::SmallDenseSet<Value> seenValues;
-    for (ShardingGroupOp& op : shardingGroupOps) {
-      if (!seenValues.insert(op.getInput()).second) {
-        op->erase();
-        op = nullptr;
+// This function verifies that sharding groups with pre-existing shardings are
+// compatible.  Compatibility means all values in the group must have either no
+// sharding or the same sharding.
+LogicalResult validateCompatibilityAndApplyInitialShardingConstraints(
+    ModuleOp module, GroupIdToShardingGroups& groupIdToShardingGroups) {
+  SmallDenseMap<int64_t, TensorShardingAttr> groupIdToSharding;
+  // Tensors can have initial shardings defined in several ways (e.g., sharding
+  // constraints, function arguments, manual computations). These initial
+  // shardings only conflict with Sharding Groups if their value belongs to a
+  // group. Therefore, we only need to validate the consistency of shardings
+  // within ShardingGroupOps to ensure no conflicts.
+  for (const auto& shardingGroups : groupIdToShardingGroups) {
+    for (ShardingGroupOp shardingGroupOp : shardingGroups) {
+      TensorShardingAttr sharding = getSharding(shardingGroupOp.getInput());
+      int64_t groupId = shardingGroupOp.getGroupId();
+      if (!sharding) {
+        continue;
+      }
+      auto [it, inserted] = groupIdToSharding.try_emplace(groupId, sharding);
+      if (!inserted && it->second != sharding) {
+        shardingGroupOp.emitError(
+            "Inconsistent shardings prior to propagation for ShardingGroupOps "
+            "with canonicalized groupId: ")
+            << groupId;
+        return failure();
       }
     }
-    llvm::erase_if(shardingGroupOps, [](ShardingGroupOp op) { return !op; });
   }
-}
 
-struct CommonSharding {
-  TensorShardingAttr sharding;
-  bool allMatchingShardings;
-};
-
-// Find the common sharding for the given sharding group. Also returns a bool
-// indicating whether all shardings in the group match. If they don't, the
-// returned sharding is the sharding of the first value in the group.
-CommonSharding findCommonSharding(
-    int64_t groupId, MutableArrayRef<ShardingGroupOp> shardingGroupOps,
-    const SymbolTable& symbolTable) {
-  if (shardingGroupOps.size() == 1) {
-    return {getSharding(shardingGroupOps.front().getInput()), true};
-  }
-  ShardingGroupOp firstShardingGroupOp = shardingGroupOps.front();
-  TensorShardingAttr sharding;
-  for (ShardingGroupOp shardingGroupOp : shardingGroupOps) {
-    TensorShardingAttr candidateSharding =
-        getSharding(shardingGroupOp.getInput());
-    if (!candidateSharding) {
-      continue;
-    }
-    if (!sharding) {
-      sharding = candidateSharding;
-    } else if (candidateSharding && sharding != candidateSharding) {
-      // TODO(bartchr): Revisit using jax.shard_alike as the example once others
-      // like PyTorch use Shardy.
-      firstShardingGroupOp.emitWarning(
-          "The initial operand shardings on the sharding groups of groupID: ")
-          << groupId << " do not match. Inserting an open "
-          << "sharding constraint to all constrained values. "
-          << "This can be caused when shardings from different values are "
-          << "grouped (e.g. from jax.shard_alike) but have separate "
-          << "inconsistent sharding constraints on them.";
-      return {sharding, false};
+  // Apply initial shardings to all values in the group.
+  for (auto& [groupId, sharding] : groupIdToSharding) {
+    for (ShardingGroupOp shardingGroupOp : groupIdToShardingGroups[groupId]) {
+      setSharding(shardingGroupOp.getInput(), sharding);
     }
   }
 
-  return {sharding, true};
-}
-
-// Add any sharding constraint ops when there is a conflict in the initial
-// shardings of values within a sharding group, or apply the common sharding.
-//
-// If there is a conflict in the initial shardings of values within a sharding
-// group, then we insert an open sharding constraint to all values in the group.
-// This is to ensure that the group is still valid after propagation.
-void addOrApplyInitialShardingConstraints(
-    ModuleOp module, GroupIdToShardingGroups& groupIdToShardingGroups) {
-  SymbolTable symbolTable(module);
-  for (const auto& [groupId, shardingGroupOps] :
-       llvm::enumerate(groupIdToShardingGroups)) {
-    auto [sharding, allmatchingShardings] =
-        findCommonSharding(groupId, shardingGroupOps, symbolTable);
-    if (!sharding) {
-      continue;
-    }
-    if (allmatchingShardings) {
-      for (ShardingGroupOp shardingGroupOp : shardingGroupOps) {
-        setSharding(shardingGroupOp.getInput(), sharding);
-      }
-      continue;
-    }
-    // NOTE: Arbitrarily use the mesh name of `sharding`. If the one already
-    // in `groupIdToConstrainedValues` is different, then once we support
-    // propagating through different meshes, this won't be an issue. Think
-    // it's fine to not error since this is a really rare case.
-    for (ShardingGroupOp shardingGroupOp : shardingGroupOps) {
-      IRRewriter rewriter(shardingGroupOp);
-      Value value = shardingGroupOp.getInput();
-      rewriter.setInsertionPointAfterValue(value);
-      auto openShardingConstraint = rewriter.create<ShardingConstraintOp>(
-          value.getLoc(), value,
-          TensorShardingAttr::getFullyOpenLike(sharding));
-      rewriter.replaceAllUsesExcept(value, openShardingConstraint,
-                                    openShardingConstraint);
-    }
-  }
+  return success();
 }
 
 struct ShardingGroupImportPass
@@ -252,10 +188,13 @@ struct ShardingGroupImportPass
 
     GroupIdToShardingGroups groupIdToReindexedTensors =
         unifyShardingGroups(tensorToGroups);
-    // Since we may add new `ShardingConstraintOp`s in
-    // `addOrApplyInitialShardingConstraints`, erase any duplicates.
-    eraseDuplicateGroups(groupIdToReindexedTensors);
-    addOrApplyInitialShardingConstraints(module, groupIdToReindexedTensors);
+    // This pass assumes sharding constraints are already applied to values.
+    // Compatibility constraints are applied after group unification to detect
+    // conflicts within the unified groups.
+    if (failed(validateCompatibilityAndApplyInitialShardingConstraints(
+            module, groupIdToReindexedTensors))) {
+      signalPassFailure();
+    }
   }
 };
 
diff --git a/shardy/dialect/sdy/transforms/import/test/sharding_group_constraints.mlir b/shardy/dialect/sdy/transforms/import/test/sharding_group_constraints.mlir
index b23dc68..6c0ba90 100644
--- a/shardy/dialect/sdy/transforms/import/test/sharding_group_constraints.mlir
+++ b/shardy/dialect/sdy/transforms/import/test/sharding_group_constraints.mlir
@@ -183,3 +183,77 @@ func.func @main(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
   sdy.sharding_group %1 group_id = 23 : tensor<8x8x1xf32>
   func.return %0: tensor<8x8xf32>
 }
+
+// -----
+
+sdy.mesh @mesh = <["a"=2, "b"=2]>
+
+// Throw error for sharding groups which have incompatible shardings inferred
+// from initial constraints.
+func.func @error_for_incompatible_shardings_in_sharding_group(
+    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>},
+    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"b"}, {}]>}) {
+  // Sharding Group and Sharding Constraint compatibility checks happend after
+  // unification + canonicalization of group ids, which is why the group id
+  // below (555) corresponds to group id: 0 in the check-error.
+  sdy.sharding_group %arg0 group_id = 555 : tensor<8x8xf32>
+  // expected-error@below {{Inconsistent shardings prior to propagation for ShardingGroupOps with canonicalized groupId: 0}}
+  sdy.sharding_group %arg1 group_id = 555 : tensor<8x8xf32>
+  func.return
+}
+
+// -----
+
+sdy.mesh @mesh = <["a"=2, "b"=2]>
+
+// Throw error for sharding groups which have incompatible shardings inferred
+// from initial constraints.
+func.func @error_for_transitively_inferred_incompatible_shardings_in_unified_sharding_group(
+    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {"b"}]>},
+    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}) {
+
+  %0 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
+  %1 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
+
+  sdy.sharding_group %arg0 group_id = 10 : tensor<8x8xf32>
+  sdy.sharding_group %0 group_id = 10 : tensor<8x8xf32>
+  sdy.sharding_group %0 group_id = 20 : tensor<8x8xf32>
+  sdy.sharding_group %1 group_id = 20 : tensor<8x8xf32>
+
+  // The shard group below will cause the above sharding groups to be merged
+  // by transitivity this implies that all of {%arg0, %arg1, 0, 1} should have
+  // the same sharding. Note that %0 and %1 are compatible by them selves but
+  // %arg0 and %arg1 are not due to their initial shardings.
+  sdy.sharding_group %1 group_id = 30 : tensor<8x8xf32>
+  // expected-error@below {{Inconsistent shardings prior to propagation for ShardingGroupOps with canonicalized groupId: 0}}
+  sdy.sharding_group %arg1 group_id = 30 : tensor<8x8xf32>
+  func.return
+}
+
+// -----
+
+sdy.mesh @mesh = <["a"=2, "b"=2]>
+
+func.func @error_for_incompatible_shardings_in_manual_computation(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) {
+  %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{"a"}, {}]>, <@mesh, [{"b"}, {}]>] out_shardings=[<@mesh, [{"b"}, {}]>] manual_axes={} (%arg2: tensor<8x8xf32>, %arg3: tensor<8x8xf32>) {
+    sdy.sharding_group %arg2 group_id = 8675 : tensor<8x8xf32>
+    // expected-error@below {{Inconsistent shardings prior to propagation for ShardingGroupOps with canonicalized groupId: 0}}
+    sdy.sharding_group %arg3 group_id = 8675 : tensor<8x8xf32>
+    sdy.return %arg2 : tensor<8x8xf32>
+  } : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+  func.return
+}
+
+// -----
+
+sdy.mesh @mesh = <["a"=2, "b"=2]>
+
+func.func @error_for_incompatible_shardings_with_sharding_constraint(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}) {
+  %0 = stablehlo.add %arg0, %arg0 :  tensor<8x8xf32>
+  %1 = sdy.sharding_constraint %0 <@mesh, [{}, {"b"}]> :  tensor<8x8xf32>
+  sdy.sharding_group %arg0 group_id = 1000 : tensor<8x8xf32>
+  // expected-error@below {{Inconsistent shardings prior to propagation for ShardingGroupOps with canonicalized groupId: 0}}
+  sdy.sharding_group %1 group_id = 1000 : tensor<8x8xf32>
+  func.return
+}
+
diff --git a/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir b/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
index 97099a1..9fd7e88 100644
--- a/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
+++ b/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
@@ -1,5 +1,4 @@
 // RUN: sdy_opt -split-input-file %s -sdy-sharding-group-import | FileCheck %s
-// RUN: sdy_opt %s -split-input-file -sdy-sharding-group-import -verify-diagnostics
 
 // CHECK-LABEL: sharding_groups_no_overlap
 func.func @sharding_groups_no_overlap(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) {
@@ -15,6 +14,8 @@ func.func @sharding_groups_no_overlap(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>
 // CHECK-LABEL: sharding_groups_all_overlap
 func.func @sharding_groups_all_overlap(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) {
   // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg1 group_id=0 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg1 group_id=0 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 0 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 1 : tensor<4xf32>
@@ -27,6 +28,8 @@ func.func @sharding_groups_all_overlap(%arg0: tensor<4xf32>, %arg1: tensor<4xf32
 
 // CHECK-LABEL: sharding_groups_overlap_min_id_used
 func.func @sharding_groups_overlap_min_id_used(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) {
+  // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg1 group_id=0 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 0 : tensor<4xf32>
@@ -41,6 +44,8 @@ func.func @sharding_groups_overlap_min_id_used(%arg0: tensor<4xf32>, %arg1: tens
 // CHECK-LABEL: sharding_groups_mixed_overlaps
 func.func @sharding_groups_mixed_overlaps(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) {
   // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg1 group_id=1 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg1 group_id=1 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 0 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 1 : tensor<4xf32>
@@ -64,6 +69,7 @@ func.func @sharding_groups_reindexes_ids(%arg0: tensor<4xf32>, %arg1: tensor<4xf
 
 // CHECK-LABEL: sharding_groups_reindex_ordering_matches_min_element_ordering
 func.func @sharding_groups_reindex_ordering_matches_min_element_ordering(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) {
+  // CHECK: sdy.sharding_group %arg0 group_id=1 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg0 group_id=1 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg1 group_id=0 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg2 group_id=2 : tensor<4xf32>
@@ -95,57 +101,19 @@ func.func @set_existing_shardings_for_sharding_group_members(
 
 sdy.mesh @mesh = <["a"=2, "b"=2]>
 
-// Emit warning as well for sharding groups which have incompatible shardings
-// inferred from initial constraints.
-// CHECK-LABEL: add_extra_sharding_constraint_for_incompatible_shardings_in_sharding_group
-func.func @add_extra_sharding_constraint_for_incompatible_shardings_in_sharding_group(
+// CHECK-LABEL: transitively_update_shardings_for_sharding_group_members
+func.func @transitively_update_shardings_for_sharding_group_members(
     %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>},
-    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"b"}, {}]>}) {
-  // CHECK-NEXT:  %[[WSC_0:.*]] = sdy.sharding_constraint %arg1 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_1:.*]] = sdy.sharding_constraint %arg0 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_1]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_0]] group_id=0 : tensor<8x8xf32>
-  // Sharding Group and Sharding Constraint compatibility checks happend after
-  // unification + canonicalization of group ids.
-  // expected-warning@below {{The initial operand shardings on the sharding groups of groupID: 0}}
-  sdy.sharding_group %arg0 group_id = 555 : tensor<8x8xf32>
-  sdy.sharding_group %arg1 group_id = 555 : tensor<8x8xf32>
-  func.return
-}
-
-// -----
-
-sdy.mesh @mesh = <["a"=2, "b"=2]>
-
-// Emit warning as well for sharding groups which have incompatible shardings
-// inferred from initial constraints.
-// CHECK-LABEL: add_extra_sharding_constraint_for_transitively_inferred_incompatible_shardings_in_unified_sharding_group
-func.func @add_extra_sharding_constraint_for_transitively_inferred_incompatible_shardings_in_unified_sharding_group(
-    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {"b"}]>},
     %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}) {
-  // CHECK-NEXT:  %[[WSC_0:.*]] = sdy.sharding_constraint %arg1 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_1:.*]] = sdy.sharding_constraint %arg0 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[CST_0:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_2:.*]] = sdy.sharding_constraint %[[CST_0]] <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[CST_1:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_3:.*]] = sdy.sharding_constraint %[[CST_1]] <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_1]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_2]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_3]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_0]] group_id=0 : tensor<8x8xf32>
+  // CHECK: %cst = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} dense<0.000000e+00> : tensor<8x8xf32>
+  // CHECK: %cst_0 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} dense<0.000000e+00> : tensor<8x8xf32>
   %0 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
   %1 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
 
-  // expected-warning@below {{The initial operand shardings on the sharding groups of groupID: 0}}
   sdy.sharding_group %arg0 group_id = 10 : tensor<8x8xf32>
   sdy.sharding_group %0 group_id = 10 : tensor<8x8xf32>
   sdy.sharding_group %0 group_id = 20 : tensor<8x8xf32>
   sdy.sharding_group %1 group_id = 20 : tensor<8x8xf32>
-
-  // The shard group below will cause the above sharding groups to be merged
-  // by transitivity this implies that all of {%arg0, %arg1, 0, 1} should have
-  // the same sharding. Note that %0 and %1 are compatible by them selves but
-  // %arg0 and %arg1 are not due to their initial shardings.
   sdy.sharding_group %1 group_id = 30 : tensor<8x8xf32>
   sdy.sharding_group %arg1 group_id = 30 : tensor<8x8xf32>
   func.return
@@ -155,19 +123,28 @@ func.func @add_extra_sharding_constraint_for_transitively_inferred_incompatible_
 
 sdy.mesh @mesh = <["a"=2, "b"=2]>
 
-// CHECK-LABEL: add_extra_sharding_constraint_for_incompatible_shardings_in_manual_computation
-func.func @add_extra_sharding_constraint_for_incompatible_shardings_in_manual_computation(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) {
-  // CHECK-NEXT:  sdy.manual_computation
-  // CHECK-NEXT:    %[[WSC_0:.*]] = sdy.sharding_constraint %arg3 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:    %[[WSC_1:.*]] = sdy.sharding_constraint %arg2 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:    sdy.sharding_group %[[WSC_1]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:    sdy.sharding_group %[[WSC_0]] group_id=0 : tensor<8x8xf32>
-  %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{"a"}, {}]>, <@mesh, [{"b"}, {}]>] out_shardings=[<@mesh, [{"b"}, {}]>] manual_axes={} (%arg2: tensor<8x8xf32>, %arg3: tensor<8x8xf32>) {
-    // expected-warning@below {{The initial operand shardings on the sharding groups of groupID: 0}}
-    sdy.sharding_group %arg2 group_id = 8675 : tensor<8x8xf32>
-    sdy.sharding_group %arg3 group_id = 8675 : tensor<8x8xf32>
-    sdy.return %arg2 : tensor<8x8xf32>
-  } : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
+// CHECK-LABEL: set_existing_shards_for_disjoint_groups
+// CHECK-SAMEL    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}
+// CHECK-SAMEL    %arg3: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}
+func.func @set_existing_shards_for_disjoint_groups(
+    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>},
+    %arg1: tensor<8x8xf32>,
+    %arg2: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"b"}]>},
+    %arg3: tensor<8x8xf32>) {
+  // CHECK: %cst = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} dense<0.000000e+00> : tensor<8x8xf32>
+  %0 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
+  // CHECK: %cst_0 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"b"}]>]>} dense<0.000000e+00> : tensor<8x8xf32>
+  %1 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
+  // CHECK: %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<8x8xf32>
+  %2 = stablehlo.constant dense<0.0> : tensor<8x8xf32>
+
+  sdy.sharding_group %arg0 group_id = 11111 : tensor<8x8xf32>
+  sdy.sharding_group %arg1 group_id = 11111 : tensor<8x8xf32>
+  sdy.sharding_group %0 group_id = 11111 : tensor<8x8xf32>
+
+  sdy.sharding_group %arg2 group_id = 22222 : tensor<8x8xf32>
+  sdy.sharding_group %arg3 group_id = 22222 : tensor<8x8xf32>
+  sdy.sharding_group %1 group_id = 22222 : tensor<8x8xf32>
   func.return
 }
 
@@ -175,19 +152,20 @@ func.func @add_extra_sharding_constraint_for_incompatible_shardings_in_manual_co
 
 sdy.mesh @mesh = <["a"=2, "b"=2]>
 
-// CHECK-LABEL: add_extra_sharding_constraint_for_incompatible_shardings_with_sharding_constraint
-func.func @add_extra_sharding_constraint_for_incompatible_shardings_with_sharding_constraint(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}) {
-  // CHECK-NEXT:  %[[WSC_0:.*]] = sdy.sharding_constraint %arg0 <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[ADD:.*]] = stablehlo.add %[[WSC_0]], %[[WSC_0]] : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_1:.*]] = sdy.sharding_constraint %[[ADD]] <@mesh, [{}, {"b"}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_2:.*]] = sdy.sharding_constraint %[[WSC_1]] <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_0]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_2]] group_id=0 : tensor<8x8xf32>
-  %0 = stablehlo.add %arg0, %arg0 :  tensor<8x8xf32>
-  %1 = sdy.sharding_constraint %0 <@mesh, [{}, {"b"}]> :  tensor<8x8xf32>
-  // expected-warning@below {{The initial operand shardings on the sharding groups of groupID: 0}}
-  sdy.sharding_group %arg0 group_id = 1000 : tensor<8x8xf32>
-  sdy.sharding_group %1 group_id = 1000 : tensor<8x8xf32>
+// CHECK-LABEL: set_existing_shardings_in_manual_computation_op
+func.func @set_existing_shardings_in_manual_computation_op(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) {
+  %0 = sdy.manual_computation(%arg0, %arg1) in_shardings=[<@mesh, [{"a"}, {}]>, <@mesh, [{"a"}, {}]>] out_shardings=[<@mesh, [{"a"}, {}]>] manual_axes={} (%arg2: tensor<8x8xf32>, %arg3: tensor<8x8xf32>) {
+    // CHECK: %1 = stablehlo.add %arg2, %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} : tensor<8x8xf32>
+    %1 = stablehlo.add %arg2, %arg2 : tensor<8x8xf32>
+    // CHECK: %2 = stablehlo.add %arg3, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} : tensor<8x8xf32>
+    %2 = stablehlo.add %arg3, %arg3 : tensor<8x8xf32>
+
+    sdy.sharding_group %1 group_id = 1000 : tensor<8x8xf32>
+    sdy.sharding_group %2 group_id = 1000 : tensor<8x8xf32>
+    sdy.sharding_group %arg2 group_id = 1000 : tensor<8x8xf32>
+    sdy.sharding_group %arg3 group_id = 1000 : tensor<8x8xf32>
+    sdy.return %1 : tensor<8x8xf32>
+  } : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
   func.return
 }
 
@@ -195,22 +173,13 @@ func.func @add_extra_sharding_constraint_for_incompatible_shardings_with_shardin
 
 sdy.mesh @mesh = <["a"=2, "b"=2]>
 
-// CHECK-LABEL: add_extra_sharding_constraint_for_incompatible_sharding_shardings
-func.func @add_extra_sharding_constraint_for_incompatible_sharding_shardings(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-  // CHECK-NEXT:  %[[WSC_0:.*]] = sdy.sharding_constraint %arg0 <@mesh, [{}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_1:.*]] = sdy.sharding_constraint %[[WSC_0]] <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_1]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_2:.*]] = sdy.sharding_constraint %arg0 <@mesh, [{"a"}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[WSC_3:.*]] = sdy.sharding_constraint %[[WSC_2]] <@mesh, [{?}, {?}]> : tensor<8x8xf32>
-  // CHECK-NEXT:  sdy.sharding_group %[[WSC_3]] group_id=0 : tensor<8x8xf32>
-  // CHECK-NEXT:  %[[ADD:.*]] = stablehlo.add %[[WSC_1]], %[[WSC_3]] : tensor<8x8xf32>
-  // CHECK-NEXT:  return %[[ADD]] : tensor<8x8xf32>
-  %0 = sdy.sharding_constraint %arg0 <@mesh, [{}, {?}]> : tensor<8x8xf32>
-  // expected-warning@below {{The initial operand shardings on the sharding groups of groupID: 0}}
-  sdy.sharding_group %0 group_id=1183 : tensor<8x8xf32>
-
-  %1 = sdy.sharding_constraint %arg0 <@mesh, [{"a"}, {?}]> : tensor<8x8xf32>
-  sdy.sharding_group %1 group_id=1183 : tensor<8x8xf32>
-  %2 = stablehlo.add %0, %1 : tensor<8x8xf32>
-  func.return %2: tensor<8x8xf32>
+func.func @set_existing_shardings_in_groups_with_sharding_constraint(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {}]>}) {
+  %0 = stablehlo.add %arg0, %arg0 :  tensor<8x8xf32>
+  %1 = sdy.sharding_constraint %0 <@mesh, [{"a"}, {}]> :  tensor<8x8xf32>
+  // CHECK: %2 = stablehlo.add %arg0, %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a"}, {}]>]>} : tensor<8x8xf32>
+  %2 = stablehlo.add %arg0, %arg0 :  tensor<8x8xf32>
+  sdy.sharding_group %arg0 group_id = 1000 : tensor<8x8xf32>
+  sdy.sharding_group %1 group_id = 1000 : tensor<8x8xf32>
+  sdy.sharding_group %2 group_id = 1000 : tensor<8x8xf32>
+  func.return
 }
diff --git a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
index f825dec..a8ae94f 100644
--- a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
+++ b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
@@ -87,25 +87,3 @@ func.func @inlined_mesh(
     (tensor<8x8xf32>, tensor<8x16xf32>) -> tensor<8x16xf32>
   return %1 : tensor<8x16xf32>
 }
-
-// -----
-
-sdy.mesh @mesh = <["a"=2, "b"=2]>
-
-// CHECK-LABEL: add_extra_sharding_constraint_for_incompatible_sharding_shardings(
-// CHECK-SAME:      %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a", ?}, {"b", ?}]>}
-// CHECK-SAME:  ) -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"a", ?}, {"b", ?}]>}) {
-func.func @add_extra_sharding_constraint_for_incompatible_sharding_shardings(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-  // CHECK-NEXT: %[[WSC_0:.*]] = sdy.reshard %arg0 <@mesh, [{}, {"b", ?}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[WSC_1:.*]] = sdy.reshard %[[WSC_0]] <@mesh, [{"a", ?}, {"b", ?}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[WSC_2:.*]] = sdy.reshard %arg0 <@mesh, [{"a"}, {"b", ?}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[WSC_3:.*]] = sdy.reshard %[[WSC_2]] <@mesh, [{"a", ?}, {"b", ?}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[ADD:.*]] = stablehlo.add %[[WSC_1]], %[[WSC_3]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a", ?}, {"b", ?}]>]>} : tensor<8x8xf32>
-  // CHECK-NEXT: return %[[ADD]] : tensor<8x8xf32>
-  %0 = sdy.sharding_constraint %arg0 <@mesh, [{}, {"b", ?}]> : tensor<8x8xf32>
-  sdy.sharding_group %0 group_id=1183 : tensor<8x8xf32>
-  %1 = sdy.sharding_constraint %arg0 <@mesh, [{"a"}, {?}]> : tensor<8x8xf32>
-  sdy.sharding_group %1 group_id=1183 : tensor<8x8xf32>
-  %2 = stablehlo.add %0, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a", ?}, {"b", ?}]>]>} : tensor<8x8xf32>
-  func.return %2: tensor<8x8xf32>
-}
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 378ac43..509398d 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,29 +1 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/llvm/lib/CodeGen/RegAllocFast.cpp b/llvm/lib/CodeGen/RegAllocFast.cpp
---- a/llvm/lib/CodeGen/RegAllocFast.cpp
-+++ b/llvm/lib/CodeGen/RegAllocFast.cpp
-@@ -984,6 +984,7 @@
- 
-   LiveRegMap::iterator LRI = findLiveVirtReg(VirtReg);
-   MCPhysReg PhysReg;
-+  bool IsRenamable = true;
-   if (LRI != LiveVirtRegs.end() && LRI->PhysReg) {
-     PhysReg = LRI->PhysReg;
-   } else {
-@@ -997,6 +998,7 @@
-       // basic.
-       PhysReg = getErrorAssignment(*LRI, *MO.getParent(), RC);
-       LRI->Error = true;
-+      IsRenamable = false;
-     } else
-       PhysReg = AllocationOrder.front();
-   }
-@@ -1007,7 +1009,7 @@
-     MO.setSubReg(0);
-   }
-   MO.setReg(PhysReg);
--  MO.setIsRenamable(!LRI->Error);
-+  MO.setIsRenamable(IsRenamable);
- }
- 
- /// Variation of defineVirtReg() with special handling for livethrough regs
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index c9480d5..ecc2de4 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "a98c2940dbc04bf84de95cb1893694cdcbc4f5fe"
-    LLVM_SHA256 = "ef93caed2e3a7c71c88740452eba3c624a336aa1cbde1a4516a69b24781520c4"
+    LLVM_COMMIT = "0e3ba99ad65f7025d37c857f9b587b767f7709e7"
+    LLVM_SHA256 = "0c39257da8ec749025e33a1ec2439b6d080eb701947e31b1583c83c7bdee0661"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index 91dd90d..a00751a 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -43,25 +43,6 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_bounded_dynamism.mlir b/sta
    %c = stablehlo.constant dense<1> : tensor<1x?xf32, #stablehlo.bounds<?, 5>>
    return %c : tensor<1x?xf32, #stablehlo.bounds<?, 5>>
  }
-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
-@@ -924,6 +924,15 @@
-   // CHECK: %[[RES:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<7x2xf32>
-   // CHECK: return %[[RES]]
-   return %0 : tensor<7x2xf32>
-+}
-+
-+// Can't do anything with the dynamic shape, but shouldn't crash.
-+// CHECK-LABEL: @dynamic_pad
-+func.func @dynamic_pad(%arg0: tensor<?x2x3xi1>, %arg1: tensor<i1>) -> tensor<?x2x1xi1> {
-+  %0 = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
-+  // CHECK-NEXT: %[[RES:.+]] = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
-+  // CHECK-NEXT: return %[[RES]]
-+  return %0 : tensor<?x2x1xi1>
- }
- 
- // -----
 diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_legalize_qdq_to_quantized_op.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_legalize_qdq_to_quantized_op.mlir
 --- stablehlo/stablehlo/tests/transforms/stablehlo_legalize_qdq_to_quantized_op.mlir
 +++ stablehlo/stablehlo/tests/transforms/stablehlo_legalize_qdq_to_quantized_op.mlir
@@ -95,22 +76,4 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeQDQToQuantizedOp.cp
      llvm::SmallVector<Value> quantizedComputeOpOperands;
      for (const Value& operand : computeOp->getOperands()) {
        auto* definingOp = operand.getDefiningOp();
-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
-@@ -934,8 +934,12 @@
-     auto padVal = op.getPaddingValue();
- 
-     auto resultTy = cast<RankedTensorType>(op.getType());
--
--    if (cast<ShapedType>(operand.getType()).getNumElements() != 0)
-+    auto operandTy = cast<RankedTensorType>(operand.getType());
-+
-+    if (!operandTy.hasStaticShape())
-+      return rewriter.notifyMatchFailure(op, "operand shape is dynamic");
-+
-+    if (operandTy.getNumElements() != 0)
-       return rewriter.notifyMatchFailure(op, "operand is not empty tensor");
- 
-     if (resultTy.hasStaticShape()) {
 
