diff --git a/docs/sdy_dialect.md b/docs/sdy_dialect.md
index 7b9e18c..ef83d35 100755
--- a/docs/sdy_dialect.md
+++ b/docs/sdy_dialect.md
@@ -467,7 +467,6 @@ the body on any free axes - those not in the manual_axes list.
 - Elements in `in_shardings` and `out_shardings` must satisfy the constraints listed in `TensorShardingAttr`.
 - The number of global and local tensor inputs/outputs of the op region must match.
 - The manual axes must come before any free axes in each dim sharding.
-- The manual axes cannot introduce padding. Namely, the dimension size must be divisible by the corresponding manual axes size.
 - The global and local shapes of the op regions arguments/results must match.
 - No manual axes are split.
 
diff --git a/shardy/dialect/sdy/ir/ops.td b/shardy/dialect/sdy/ir/ops.td
index d598517..07bfa11 100644
--- a/shardy/dialect/sdy/ir/ops.td
+++ b/shardy/dialect/sdy/ir/ops.td
@@ -145,7 +145,6 @@ def Sdy_ManualComputationOp : Sdy_Op<"manual_computation",
     - Elements in `in_shardings` and `out_shardings` must satisfy the constraints listed in `TensorShardingAttr`.
     - The number of global and local tensor inputs/outputs of the op region must match.
     - The manual axes must come before any free axes in each dim sharding.
-    - The manual axes cannot introduce padding. Namely, the dimension size must be divisible by the corresponding manual axes size.
     - The global and local shapes of the op regions arguments/results must match.
     - No manual axes are split.
   }];
diff --git a/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir b/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
index c17ea23..139e1f2 100644
--- a/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
+++ b/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
@@ -165,19 +165,6 @@ func.func @man_comp_result_rank_mistmatch(%arg0: tensor<16x32xf32>) -> tensor<16
 
 // -----
 
-sdy.mesh @mesh = <["a"=4]>
-
-func.func @dimension_size_not_divisible_by_manual_axes_size(%arg0: tensor<6xf32>) -> tensor<6xf32> {
-  // expected-error @+1 {{dimension size 6 is not divisible by the manual axes size 4}}
-  %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{"a"}]>] out_shardings=[<@mesh, [{"a"}]>] manual_axes={"a"} (%arg1: tensor<1xf32>) {
-    %1 = stablehlo.add %arg1, %arg1 : tensor<1xf32>
-    sdy.return %1 : tensor<1xf32>
-  } : (tensor<6xf32>) -> tensor<6xf32>
-  func.return %0: tensor<6xf32>
-}
-
-// -----
-
 sdy.mesh @mesh = <["a"=2]>
 
 func.func @man_comp_operand_shape_mismatch_replicated(%arg0: tensor<16x32xf32>) -> tensor<16x32xf32> {
diff --git a/shardy/dialect/sdy/ir/verifiers.cc b/shardy/dialect/sdy/ir/verifiers.cc
index e3edcfe..87fc0ee 100644
--- a/shardy/dialect/sdy/ir/verifiers.cc
+++ b/shardy/dialect/sdy/ir/verifiers.cc
@@ -801,11 +801,9 @@ ArrayRef<AxisRefAttr>::iterator findManualAxisAfterFreeAxis(
 // 3. the number of global and local tensor inputs/outputs of the op region
 //    match,
 // 4. the manual axes come before any free axes in each dim sharding,
-// 5. The manual axes cannot introduce padding. The dimension size must be
-//    divisible by the corresponding manual axes size.
-// 6. the global shape and local shapes of the op regions arguments/results
+// 5. the global shape and local shapes of the op regions arguments/results
 //    match, and
-// 7. No manual axes are split.
+// 6. No manual axes are split.
 //
 // `valueKindStr` is a string included in any verification error message
 // specifying whether the values we are verifying are the operands or results.
@@ -864,6 +862,8 @@ LogicalResult verifyManualComputationValue(
       }
     }
 
+    // 5. Verify the global shape and local shapes of the op regions
+    //    arguments/results match.
     SmallVector<int64_t> newDimSizes;
     auto globalRankedType = mlir::cast<RankedTensorType>(globalType);
     for (auto [dimensionSize, dimSharding] : llvm::zip_equal(
@@ -871,24 +871,13 @@ LogicalResult verifyManualComputationValue(
       if (dimensionSize == ShapedType::kDynamic) {
         newDimSizes.push_back(ShapedType::kDynamic);
       } else {
-        // 5. The manual axes cannot introduce padding. The dimension size must
-        //    be divisible by the corresponding manual axes size.
-
         // Safe to call `getMesh` because the sharding was already verified.
-        int64_t manualAxesSize =
+        newDimSizes.push_back(
+            dimensionSize /
             accumulatedManualAxesSize(op, dimSharding.getAxes(), manualAxesSet,
-                                      sharding.getMesh(symbolTable));
-        if (dimensionSize % manualAxesSize != 0) {
-          return op->emitOpError(valueKindStr)
-                 << " dimension size " << dimensionSize
-                 << " is not divisible by the manual axes size "
-                 << manualAxesSize;
-        }
-        newDimSizes.push_back(dimensionSize / manualAxesSize);
+                                      sharding.getMesh(symbolTable)));
       }
     }
-    // 6. Verify the global shape and local shapes of the op regions
-    //    arguments/results match.
     auto expectedLocalRankedType =
         RankedTensorType::get(newDimSizes, globalRankedType.getElementType());
     auto localRankedType = mlir::cast<RankedTensorType>(localType);
@@ -900,7 +889,7 @@ LogicalResult verifyManualComputationValue(
              << ", actual local shape " << localRankedType;
     }
 
-    // 7. No manual axes are split.
+    // 6. No manual axes are split.
     if (sharding.anyOfAxisRef([&](AxisRefAttr axis) {
           return axis.getSubAxisInfo() &&
                  manualAxesSet.contains(axis.getName());
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
index 96c631a..a4ca997 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
@@ -1716,7 +1716,7 @@ func.func @manual_computation(%arg0: tensor<210xf32> {sdy.sharding = #sdy.shardi
 }
 
 // CHECK-LABEL: func @manual_computation_with_manual_axes
-func.func @manual_computation_with_manual_axes(%arg0: tensor<208xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x","y"}]>}) -> (tensor<208xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x","z"}]>}) {
+func.func @manual_computation_with_manual_axes(%arg0: tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x","y"}]>}) -> (tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x","z"}]>}) {
   %0 = sdy.manual_computation(%arg0)
     in_shardings=[<@mesh_xyzt, [{"x","y"}]>] out_shardings=[<@mesh_xyzt, [{"x", "z"}]>] manual_axes={"x"} (%arg1: tensor<52xf32>) {
     // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh_xyzt, [{"t"}]> : tensor<52xf32>
@@ -1725,9 +1725,9 @@ func.func @manual_computation_with_manual_axes(%arg0: tensor<208xf32> {sdy.shard
     // CHECK-NEXT: sdy.return %[[RESHARD2]] : tensor<52xf32>
     %2 = stablehlo.abs %arg1 {sdy.sharding=#sdy.sharding_per_value<[<@mesh_xyzt, [{"t"}]>]>} : tensor<52xf32>
     sdy.return %2 : tensor<52xf32>
-  } : (tensor<208xf32>) -> (tensor<208xf32>)
-  %1 = stablehlo.negate %0 {sdy.sharding= #sdy.sharding_per_value<[<@mesh_xyzt, [{"x","z"}]>]>} : tensor<208xf32>
-  return %1 : tensor<208xf32>
+  } : (tensor<210xf32>) -> (tensor<210xf32>)
+  %1 = stablehlo.negate %0 {sdy.sharding= #sdy.sharding_per_value<[<@mesh_xyzt, [{"x","z"}]>]>} : tensor<210xf32>
+  return %1 : tensor<210xf32>
 }
 
 // CHECK-LABEL: func @optimization_barrier
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 436c4e9..2337741 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -14,51 +14,6 @@ diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaExprCXX.cpp b/clang/lib/Sema/
  }
  
  /// Select the correct "usual" deallocation function to use from a selection of
-diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderStmt.cpp b/clang/lib/Serialization/ASTReaderStmt.cpp
---- a/clang/lib/Serialization/ASTReaderStmt.cpp
-+++ b/clang/lib/Serialization/ASTReaderStmt.cpp
-@@ -2226,10 +2226,7 @@
-   E->AssociatedDeclAndRef.setPointer(readDeclAs<Decl>());
-   E->AssociatedDeclAndRef.setInt(CurrentUnpackingBits->getNextBit());
-   E->Index = CurrentUnpackingBits->getNextBits(/*Width=*/12);
--  if (CurrentUnpackingBits->getNextBit())
--    E->PackIndex = Record.readInt();
--  else
--    E->PackIndex = 0;
-+  E->PackIndex = Record.readUnsignedOrNone().toInternalRepresentation();
-   E->Final = CurrentUnpackingBits->getNextBit();
-   E->SubstNonTypeTemplateParmExprBits.NameLoc = readSourceLocation();
-   E->Replacement = Record.readSubExpr();
-@@ -2239,6 +2236,7 @@
-                                           SubstNonTypeTemplateParmPackExpr *E) {
-   VisitExpr(E);
-   E->AssociatedDecl = readDeclAs<Decl>();
-+  E->Final = CurrentUnpackingBits->getNextBit();
-   E->Index = Record.readInt();
-   TemplateArgument ArgPack = Record.readTemplateArgument();
-   if (ArgPack.getKind() != TemplateArgument::Pack)
-diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTWriterStmt.cpp b/clang/lib/Serialization/ASTWriterStmt.cpp
---- a/clang/lib/Serialization/ASTWriterStmt.cpp
-+++ b/clang/lib/Serialization/ASTWriterStmt.cpp
-@@ -2228,9 +2228,7 @@
-   Record.AddDeclRef(E->getAssociatedDecl());
-   CurrentPackingBits.addBit(E->isReferenceParameter());
-   CurrentPackingBits.addBits(E->getIndex(), /*Width=*/12);
--  CurrentPackingBits.addBit((bool)E->getPackIndex());
--  if (auto PackIndex = E->getPackIndex())
--    Record.push_back(*PackIndex + 1);
-+  Record.writeUnsignedOrNone(E->getPackIndex());
-   CurrentPackingBits.addBit(E->getFinal());
- 
-   Record.AddSourceLocation(E->getNameLoc());
-@@ -2242,6 +2240,7 @@
-                                           SubstNonTypeTemplateParmPackExpr *E) {
-   VisitExpr(E);
-   Record.AddDeclRef(E->getAssociatedDecl());
-+  CurrentPackingBits.addBit(E->getFinal());
-   Record.push_back(E->getIndex());
-   Record.AddTemplateArgument(E->getArgumentPack());
-   Record.AddSourceLocation(E->getParameterPackLocation());
 diff -ruN --strip-trailing-cr a/clang/test/CodeGenCXX/bug135668.cpp b/clang/test/CodeGenCXX/bug135668.cpp
 --- a/clang/test/CodeGenCXX/bug135668.cpp
 +++ b/clang/test/CodeGenCXX/bug135668.cpp
@@ -130,3 +85,294 @@ diff -ruN --strip-trailing-cr a/clang/test/SemaCXX/bug135668.cpp b/clang/test/Se
 +  TestClass *obj = new TestClass() ;
 +  return obj->field;
 +}
+diff -ruN --strip-trailing-cr a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+--- a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
++++ b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+@@ -25183,7 +25183,7 @@
+     return SDValue();
+ 
+   auto *Ld = dyn_cast<LoadSDNode>(Extract->getOperand(0));
+-  if (!Ld || Ld->getExtensionType() || !Ld->isSimple())
++  if (!Ld || !ISD::isNormalLoad(Ld) || !Ld->isSimple())
+     return SDValue();
+ 
+   // Allow targets to opt-out.
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+--- a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
++++ b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+@@ -7241,6 +7241,8 @@
+             return Res.takeVector();
+           };
+           auto GetNumOperands = [](const TreeEntry *TE) {
++            if (TE->State == TreeEntry::SplitVectorize)
++              return TE->getNumOperands();
+             if (auto *CI = dyn_cast<CallInst>(TE->getMainOp()); CI)
+               return CI->arg_size();
+             return TE->getNumOperands();
+@@ -18064,8 +18066,14 @@
+   // need to rebuild it.
+   EntryToLastInstruction.clear();
+   // All blocks must be scheduled before any instructions are inserted.
+-  for (auto &BSIter : BlocksSchedules) {
++  for (auto &BSIter : BlocksSchedules)
+     scheduleBlock(BSIter.second.get());
++  // Cache last instructions for the nodes to avoid side effects, which may
++  // appear during vectorization, like extra uses, etc.
++  for (const std::unique_ptr<TreeEntry> &TE : VectorizableTree) {
++    if (TE->isGather())
++      continue;
++    (void)getLastInstructionInBundle(TE.get());
+   }
+ 
+   if (ReductionRoot)
+diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/AArch64/pr135821.ll b/llvm/test/CodeGen/AArch64/pr135821.ll
+--- a/llvm/test/CodeGen/AArch64/pr135821.ll
++++ b/llvm/test/CodeGen/AArch64/pr135821.ll
+@@ -0,0 +1,27 @@
++; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
++; RUN: llc < %s -mtriple=aarch64-unknown-linux-gnu  | FileCheck %s
++
++define <4 x float> @f(ptr  %0) {
++; CHECK-LABEL: f:
++; CHECK:       // %bb.0:
++; CHECK-NEXT:    sub sp, sp, #32
++; CHECK-NEXT:    str x30, [sp, #16] // 8-byte Folded Spill
++; CHECK-NEXT:    .cfi_def_cfa_offset 32
++; CHECK-NEXT:    .cfi_offset w30, -16
++; CHECK-NEXT:    ldr q1, [x0, #56]!
++; CHECK-NEXT:    ldr d0, [x0, #16]
++; CHECK-NEXT:    mov v1.d[1], v0.d[0]
++; CHECK-NEXT:    str q1, [sp] // 16-byte Folded Spill
++; CHECK-NEXT:    bl use
++; CHECK-NEXT:    ldr q0, [sp] // 16-byte Folded Reload
++; CHECK-NEXT:    ldr x30, [sp, #16] // 8-byte Folded Reload
++; CHECK-NEXT:    add sp, sp, #32
++; CHECK-NEXT:    ret
++  %2 = getelementptr inbounds nuw i8, ptr %0, i64 56
++  %3 = load <6 x float>, ptr %2, align 4
++  %4 = shufflevector <6 x float> %3, <6 x float> poison, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
++  tail call void @use(ptr %2)
++  ret <4 x float> %4
++}
++
++declare void @use(ptr)
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll b/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
+--- a/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
++++ b/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
+@@ -0,0 +1,91 @@
++; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
++; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-generic-linux-gnu < %s | FileCheck %s
++
++define void @test(ptr %nExp, float %0, i1 %cmp, float %1) {
++; CHECK-LABEL: define void @test(
++; CHECK-SAME: ptr [[NEXP:%.*]], float [[TMP0:%.*]], i1 [[CMP:%.*]], float [[TMP1:%.*]]) {
++; CHECK-NEXT:  [[ENTRY:.*]]:
++; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <4 x float> <float 0.000000e+00, float 0x7FF8000000000000, float poison, float poison>, float [[TMP1]], i32 2
++; CHECK-NEXT:    [[TMP3:%.*]] = insertelement <4 x float> [[TMP2]], float [[TMP0]], i32 3
++; CHECK-NEXT:    br i1 [[CMP]], label %[[IF_THEN:.*]], label %[[IF_END:.*]]
++; CHECK:       [[IF_THEN]]:
++; CHECK-NEXT:    [[TMP4:%.*]] = load float, ptr [[NEXP]], align 4
++; CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <4 x float> [[TMP3]], <4 x float> poison, <2 x i32> <i32 3, i32 3>
++; CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x float> [[TMP5]], float [[TMP4]], i32 0
++; CHECK-NEXT:    [[TMP7:%.*]] = fmul <2 x float> [[TMP6]], zeroinitializer
++; CHECK-NEXT:    [[TMP8:%.*]] = fmul <2 x float> [[TMP5]], zeroinitializer
++; CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x float> <float poison, float 0.000000e+00, float 0.000000e+00, float poison>, float [[TMP1]], i32 3
++; CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <2 x float> [[TMP8]], <2 x float> poison, <4 x i32> <i32 0, i32 poison, i32 poison, i32 poison>
++; CHECK-NEXT:    [[TMP11:%.*]] = shufflevector <4 x float> [[TMP9]], <4 x float> [[TMP10]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    br label %[[IF_END]]
++; CHECK:       [[IF_END]]:
++; CHECK-NEXT:    [[TMP12:%.*]] = phi <4 x float> [ [[TMP11]], %[[IF_THEN]] ], [ [[TMP3]], %[[ENTRY]] ]
++; CHECK-NEXT:    [[TMP13:%.*]] = phi <2 x float> [ [[TMP8]], %[[IF_THEN]] ], [ zeroinitializer, %[[ENTRY]] ]
++; CHECK-NEXT:    [[TMP14:%.*]] = phi <2 x float> [ zeroinitializer, %[[IF_THEN]] ], [ <float 0x7FF8000000000000, float 1.000000e+00>, %[[ENTRY]] ]
++; CHECK-NEXT:    [[TMP15:%.*]] = phi <2 x float> [ [[TMP7]], %[[IF_THEN]] ], [ zeroinitializer, %[[ENTRY]] ]
++; CHECK-NEXT:    [[TMP16:%.*]] = shufflevector <2 x float> [[TMP14]], <2 x float> <float poison, float 0.000000e+00>, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[TMP17:%.*]] = fmul <2 x float> [[TMP15]], [[TMP16]]
++; CHECK-NEXT:    [[TMP18:%.*]] = fmul <2 x float> [[TMP13]], [[TMP14]]
++; CHECK-NEXT:    [[TMP19:%.*]] = fmul <4 x float> [[TMP12]], zeroinitializer
++; CHECK-NEXT:    [[CALL25:%.*]] = load volatile ptr, ptr null, align 8
++; CHECK-NEXT:    [[TMP20:%.*]] = fadd <2 x float> [[TMP18]], [[TMP17]]
++; CHECK-NEXT:    [[TMP21:%.*]] = fmul <2 x float> [[TMP20]], zeroinitializer
++; CHECK-NEXT:    [[TMP22:%.*]] = fadd <2 x float> [[TMP21]], zeroinitializer
++; CHECK-NEXT:    [[TMP23:%.*]] = fmul <4 x float> [[TMP19]], zeroinitializer
++; CHECK-NEXT:    [[TMP24:%.*]] = fadd <4 x float> [[TMP19]], zeroinitializer
++; CHECK-NEXT:    [[TMP25:%.*]] = shufflevector <4 x float> [[TMP23]], <4 x float> [[TMP24]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
++; CHECK-NEXT:    [[TMP26:%.*]] = call <4 x float> @llvm.vector.insert.v4f32.v2f32(<4 x float> <float 0.000000e+00, float 1.000000e+00, float poison, float poison>, <2 x float> [[TMP22]], i64 2)
++; CHECK-NEXT:    [[TMP27:%.*]] = fadd <4 x float> [[TMP25]], [[TMP26]]
++; CHECK-NEXT:    store <4 x float> [[TMP27]], ptr [[CALL25]], align 4
++; CHECK-NEXT:    ret void
++;
++entry:
++  br i1 %cmp, label %if.then, label %if.end
++
++if.then:
++  %div.i41 = fmul float %0, 0.000000e+00
++  %2 = load float, ptr %nExp, align 4
++  %div.1.i.i = fmul float %2, 0.000000e+00
++  %div.2.i.i = fmul float %0, 0.000000e+00
++  br label %if.end
++
++if.end:
++  %3 = phi float [ %1, %if.then ], [ %0, %entry ]
++  %4 = phi float [ 0.000000e+00, %if.then ], [ %1, %entry ]
++  %5 = phi float [ 0.000000e+00, %if.then ], [ 0x7FF8000000000000, %entry ]
++  %6 = phi float [ 0.000000e+00, %if.then ], [ 1.000000e+00, %entry ]
++  %fa.sroa.9.0 = phi float [ %div.2.i.i, %if.then ], [ 0.000000e+00, %entry ]
++  %fa.sroa.7.0 = phi float [ %div.1.i.i, %if.then ], [ 0.000000e+00, %entry ]
++  %fa.sroa.0.0 = phi float [ %div.i41, %if.then ], [ 0.000000e+00, %entry ]
++  %mul.1.i.i58 = fmul float %fa.sroa.7.0, %6
++  %mul.2.i.i60 = fmul float %fa.sroa.9.0, %6
++  %mul.1.i.i.i63 = fmul float %fa.sroa.0.0, %5
++  %mul.2.i.i.i65 = fmul float %fa.sroa.0.0, 0.000000e+00
++  %mul.i66 = fmul float %fa.sroa.0.0, 0.000000e+00
++  %add.1.i.i = fadd float %mul.1.i.i58, %mul.1.i.i.i63
++  %add.2.i.i = fadd float %mul.2.i.i60, %mul.2.i.i.i65
++  %mul.1.i.i74 = fmul float %add.1.i.i, 0.000000e+00
++  %mul.2.i.i76 = fmul float %add.2.i.i, 0.000000e+00
++  %mul.i.i.i78 = fmul float %mul.i66, 0.000000e+00
++  %add.1.i.i85 = fadd float %mul.1.i.i74, 0.000000e+00
++  %add.2.i.i86 = fadd float %mul.2.i.i76, 0.000000e+00
++  %mul.i.i.i97 = fmul float %5, 0.000000e+00
++  %mul.1.i.i.i99 = fmul float %4, 0.000000e+00
++  %mul.2.i.i.i101 = fmul float %3, 0.000000e+00
++  %add.i.i103 = fadd float %mul.i.i.i97, 0.000000e+00
++  %add.1.i.i104 = fadd float %mul.1.i.i.i99, 0.000000e+00
++  %add.2.i.i105 = fadd float %mul.2.i.i.i101, 0.000000e+00
++  %add = fadd float %mul.i.i.i78, 0.000000e+00
++  %add.i = fadd float %add.i.i103, 1.000000e+00
++  %add.1.i = fadd float %add.1.i.i104, %add.1.i.i85
++  %add.2.i = fadd float %add.2.i.i105, %add.2.i.i86
++  %call25 = load volatile ptr, ptr null, align 8
++  store float %add, ptr %call25, align 4
++  %__trans_tmp_29.sroa.5.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 4
++  store float %add.i, ptr %__trans_tmp_29.sroa.5.0.call25.sroa_idx, align 4
++  %__trans_tmp_29.sroa.6.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 8
++  store float %add.1.i, ptr %__trans_tmp_29.sroa.6.0.call25.sroa_idx, align 4
++  %__trans_tmp_29.sroa.7.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 12
++  store float %add.2.i, ptr %__trans_tmp_29.sroa.7.0.call25.sroa_idx, align 4
++  ret void
++}
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll b/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
+--- a/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
++++ b/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
+@@ -0,0 +1,121 @@
++; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
++; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-unknown-linux-gnu -mattr=+avx -slp-threshold=-1000 < %s | FileCheck %s
++
++define i64 @Foo(ptr align 8 dereferenceable(344) %0, i64 %1) {
++; CHECK-LABEL: define i64 @Foo(
++; CHECK-SAME: ptr align 8 dereferenceable(344) [[TMP0:%.*]], i64 [[TMP1:%.*]]) #[[ATTR0:[0-9]+]] {
++; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[TMP0]], i64 104
++; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[TMP0]], i64 112
++; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[TMP0]], i64 24
++; CHECK-NEXT:    [[TMP6:%.*]] = load i64, ptr [[TMP3]], align 8
++; CHECK-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP4]], align 8
++; CHECK-NEXT:    [[TMP8:%.*]] = load i64, ptr [[TMP5]], align 8
++; CHECK-NEXT:    [[TMP9:%.*]] = load i64, ptr [[TMP0]], align 8
++; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <2 x i64> poison, i64 [[TMP6]], i32 0
++; CHECK-NEXT:    [[TMP11:%.*]] = insertelement <2 x i64> [[TMP10]], i64 [[TMP9]], i32 1
++; CHECK-NEXT:    [[TMP12:%.*]] = insertelement <2 x i64> poison, i64 [[TMP7]], i32 0
++; CHECK-NEXT:    [[TMP13:%.*]] = insertelement <2 x i64> [[TMP12]], i64 [[TMP8]], i32 1
++; CHECK-NEXT:    [[TMP14:%.*]] = insertelement <2 x i64> poison, i64 0, i32 0
++; CHECK-NEXT:    [[TMP15:%.*]] = insertelement <2 x i64> <i64 0, i64 poison>, i64 [[TMP1]], i32 1
++; CHECK-NEXT:    br label %[[BB16:.*]]
++; CHECK:       [[BB16]]:
++; CHECK-NEXT:    [[TMP17:%.*]] = phi <2 x i64> [ [[TMP11]], [[TMP2:%.*]] ], [ zeroinitializer, %[[TMP25:.*]] ]
++; CHECK-NEXT:    [[TMP18:%.*]] = phi <2 x i64> [ [[TMP13]], [[TMP2]] ], [ [[TMP29:%.*]], %[[TMP25]] ]
++; CHECK-NEXT:    switch i32 0, label %[[BB19:.*]] [
++; CHECK-NEXT:      i32 0, label %[[TMP25]]
++; CHECK-NEXT:    ]
++; CHECK:       [[BB19]]:
++; CHECK-NEXT:    [[TMP20:%.*]] = shufflevector <2 x i64> [[TMP18]], <2 x i64> poison, <4 x i32> <i32 1, i32 poison, i32 poison, i32 poison>
++; CHECK-NEXT:    [[TMP21:%.*]] = insertelement <4 x i64> [[TMP20]], i64 0, i32 1
++; CHECK-NEXT:    [[TMP22:%.*]] = insertelement <4 x i64> [[TMP21]], i64 0, i32 2
++; CHECK-NEXT:    [[TMP23:%.*]] = shufflevector <4 x i64> [[TMP22]], <4 x i64> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 2>
++; CHECK-NEXT:    [[TMP24:%.*]] = shufflevector <2 x i64> [[TMP14]], <2 x i64> [[TMP18]], <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    br label %[[TMP25]]
++; CHECK:       [[TMP25]]:
++; CHECK-NEXT:    [[TMP26:%.*]] = phi <2 x i64> [ [[TMP17]], %[[BB19]] ], [ zeroinitializer, %[[BB16]] ]
++; CHECK-NEXT:    [[TMP27:%.*]] = phi <4 x i64> [ [[TMP23]], %[[BB19]] ], [ zeroinitializer, %[[BB16]] ]
++; CHECK-NEXT:    [[TMP28:%.*]] = phi <2 x i64> [ [[TMP24]], %[[BB19]] ], [ [[TMP15]], %[[BB16]] ]
++; CHECK-NEXT:    [[TMP29]] = shufflevector <2 x i64> [[TMP18]], <2 x i64> <i64 0, i64 poison>, <2 x i32> <i32 2, i32 1>
++; CHECK-NEXT:    br i1 false, label %[[DOTLOOPEXIT206:.*]], label %[[BB16]]
++; CHECK:       [[_LOOPEXIT206:.*:]]
++; CHECK-NEXT:    switch i32 0, label %[[BB32:.*]] [
++; CHECK-NEXT:      i32 0, [[DOTCONT174:label %.*]]
++; CHECK-NEXT:      i32 1, label %[[BB30:.*]]
++; CHECK-NEXT:    ]
++; CHECK:       [[BB30]]:
++; CHECK-NEXT:    [[TMP31:%.*]] = shufflevector <4 x i64> [[TMP27]], <4 x i64> <i64 0, i64 0, i64 poison, i64 0>, <4 x i32> <i32 4, i32 5, i32 2, i32 7>
++; CHECK-NEXT:    br [[DOTCONT174]]
++; CHECK:       [[BB32]]:
++; CHECK-NEXT:    [[TMP33:%.*]] = insertelement <4 x i64> [[TMP27]], i64 0, i32 1
++; CHECK-NEXT:    [[TMP34:%.*]] = insertelement <4 x i64> [[TMP33]], i64 0, i32 2
++; CHECK-NEXT:    [[TMP35:%.*]] = shufflevector <4 x i64> [[TMP34]], <4 x i64> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 2>
++; CHECK-NEXT:    [[TMP36:%.*]] = insertelement <2 x i64> [[TMP28]], i64 0, i32 0
++; CHECK-NEXT:    br [[DOTCONT174]]
++; CHECK:       [[_CONT174:.*:]]
++; CHECK-NEXT:    [[TMP37:%.*]] = phi <2 x i64> [ [[TMP26]], %[[BB32]] ], [ zeroinitializer, %[[BB30]] ], [ [[TMP26]], %[[DOTLOOPEXIT206]] ]
++; CHECK-NEXT:    [[TMP38:%.*]] = phi <4 x i64> [ [[TMP35]], %[[BB32]] ], [ [[TMP31]], %[[BB30]] ], [ [[TMP27]], %[[DOTLOOPEXIT206]] ]
++; CHECK-NEXT:    [[TMP39:%.*]] = phi <2 x i64> [ [[TMP36]], %[[BB32]] ], [ zeroinitializer, %[[BB30]] ], [ [[TMP28]], %[[DOTLOOPEXIT206]] ]
++; CHECK-NEXT:    ret i64 0
++;
++  %3 = getelementptr i8, ptr %0, i64 104
++  %4 = getelementptr i8, ptr %0, i64 112
++  %5 = getelementptr i8, ptr %0, i64 24
++  %6 = load i64, ptr %3, align 8
++  %7 = load i64, ptr %4, align 8
++  %8 = load i64, ptr %5, align 8
++  %9 = load i64, ptr %0, align 8
++  br label %10
++
++10:
++  %11 = phi i64 [ %9, %2 ], [ 0, %18 ]
++  %12 = phi i64 [ %8, %2 ], [ %12, %18 ]
++  %13 = phi i64 [ %7, %2 ], [ 0, %18 ]
++  %14 = phi i64 [ %6, %2 ], [ 0, %18 ]
++  switch i32 0, label %15 [
++  i32 0, label %18
++  ]
++
++15:
++  %16 = tail call i64 @llvm.umin.i64(i64 0, i64 0)
++  %17 = tail call i64 @llvm.umax.i64(i64 0, i64 0)
++  br label %18
++
++18:
++  %19 = phi i64 [ %17, %15 ], [ 0, %10 ]
++  %20 = phi i64 [ %16, %15 ], [ 0, %10 ]
++  %21 = phi i64 [ %11, %15 ], [ 0, %10 ]
++  %22 = phi i64 [ %12, %15 ], [ 0, %10 ]
++  %23 = phi i64 [ %13, %15 ], [ %1, %10 ]
++  %24 = phi i64 [ %14, %15 ], [ 0, %10 ]
++  br i1 false, label %.loopexit206, label %10
++
++.loopexit206:
++  switch i32 0, label %26 [
++  i32 0, label %.cont174
++  i32 1, label %25
++  ]
++
++25:
++  br label %.cont174
++
++26:
++  %27 = tail call i64 @llvm.umin.i64(i64 0, i64 0)
++  %28 = tail call i64 @llvm.umax.i64(i64 0, i64 0)
++  br label %.cont174
++
++.cont174:
++  %.sroa.139.1 = phi i64 [ %28, %26 ], [ %19, %25 ], [ %19, %.loopexit206 ]
++  %.sroa.133.1 = phi i64 [ %27, %26 ], [ 0, %25 ], [ %20, %.loopexit206 ]
++  %.sroa.81.1 = phi i64 [ %23, %26 ], [ 0, %25 ], [ %23, %.loopexit206 ]
++  %.sroa.75.1 = phi i64 [ %24, %26 ], [ 0, %25 ], [ %24, %.loopexit206 ]
++  %.sroa.21.1 = phi i64 [ %21, %26 ], [ 0, %25 ], [ %21, %.loopexit206 ]
++  %.sroa.15.1 = phi i64 [ %22, %26 ], [ 0, %25 ], [ %22, %.loopexit206 ]
++  %29 = phi i64 [ %28, %26 ], [ 0, %25 ], [ %19, %.loopexit206 ]
++  %30 = phi i64 [ %27, %26 ], [ 0, %25 ], [ %20, %.loopexit206 ]
++  ret i64 0
++}
++
++declare i64 @llvm.umax.i64(i64, i64)
++
++declare i64 @llvm.umin.i64(i64, i64)
++
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index 3ec4c3e..d44a9f6 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "179d30f8c3fddd3c85056fd2b8e877a4a8513158"
-    LLVM_SHA256 = "39f33d0ba77ca40d254c767519a0f3f5692c2caa271f413e7245ab63d0787bd5"
+    LLVM_COMMIT = "ffd5b148941a1146378a247c70c4faface3a1f96"
+    LLVM_SHA256 = "fc57e9b703ddfb6d888e1c5beb2a65ca8d84d439bcf88c63eb014ccb8bbea414"
 
     tf_http_archive(
         name = name,
