diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
@@ -1051,9 +1051,9 @@
   %0 = "stablehlo.reshape"(%arg0) : (tensor<1x?xi32>) -> tensor<1xi32>
   func.return %0 : tensor<1xi32>
 }
-// CHECK: %[[CAST:.*]] = tensor.cast %{{.*}} : tensor<1x?xi32> to tensor<1x1xi32>
-// CHECK: %[[COLLAPSE:.*]] = tensor.collapse_shape %[[CAST]] {{\[}}[0, 1]] : tensor<1x1xi32> into tensor<1xi32>
-// CHECK: return %[[COLLAPSE:.*]] : tensor<1xi32>
+// CHECK: %[[COLLAPSE:.*]] = tensor.collapse_shape %arg0 {{\[}}[0, 1]] : tensor<1x?xi32> into tensor<?xi32>
+// CHECK: %[[CAST:.*]] = tensor.cast %[[COLLAPSE]] : tensor<?xi32> to tensor<1xi32>
+// CHECK: return %[[CAST]] : tensor<1xi32>
 
 // -----
 
@@ -1084,9 +1084,10 @@
   %0 = "stablehlo.reshape"(%arg0) : (tensor<16x1x?xi32>) -> tensor<16xi32>
   func.return %0 : tensor<16xi32>
 }
-// CHECK: %[[CAST:.*]] = tensor.cast %{{.*}} : tensor<16x1x?xi32> to tensor<16x1x1xi32>
-// CHECK: %[[COLLAPSE:.*]] = tensor.collapse_shape %[[CAST]] {{\[}}[0, 1, 2]] : tensor<16x1x1xi32> into tensor<16xi32>
-// CHECK: return %[[COLLAPSE:.*]] : tensor<16xi32>
+
+// CHECK: %[[COLLAPSE:.*]] = tensor.collapse_shape %arg0 {{\[}}[0, 1, 2]] : tensor<16x1x?xi32> into tensor<?xi32>
+// CHECK: %[[CAST:.*]] = tensor.cast %[[COLLAPSE]] : tensor<?xi32> to tensor<16xi32>
+// CHECK: return %[[CAST]] : tensor<16xi32>
 
 // -----
 
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -30,6 +30,7 @@
 #include "llvm/ADT/SetVector.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/SmallVectorExtras.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/Casting.h"
 #include "mlir/Dialect/Arith/IR/Arith.h"
 #include "mlir/Dialect/Arith/Utils/Utils.h"
@@ -72,6 +73,8 @@
 #include "stablehlo/conversions/linalg/transforms/TypeConversion.h"
 #include "stablehlo/dialect/StablehloOps.h"
 
+#define DEBUG_TYPE "stablehlo-legalize-to-linalg"
+
 namespace mlir::stablehlo {
 
 #define GEN_PASS_DEF_STABLEHLOLEGALIZETOLINALGPASS
@@ -1127,6 +1130,10 @@
     // collapse_shape.
     if (std::optional<SmallVector<ReassociationIndices>> reassociationMap =
             getReassociationIndicesForReshape(operandType, resultType)) {
+      LLVM_DEBUG(llvm::dbgs() << "ReshapeOp: " << operandType << " -> "
+                              << resultType << " \n"
+                              << "  using reassociation map: "
+                              << reassociationMap.value().size() << "\n");
       if (resultType.getRank() < operandType.getRank()) {
         // We have found a working reassociation map. If the operand is dynamic,
         // we first need to cast all unknown dimensions in the input that get
@@ -1134,9 +1141,10 @@
         SmallVector<int64_t> shape(operandType.getShape().begin(),
                                    operandType.getShape().end());
         for (auto [idx, dims] : llvm::enumerate(*reassociationMap)) {
-          // If the result dim is dynamic, we do not mind dynamic entries in the
-          // source.
-          if (resultType.isDynamicDim(idx)) continue;
+          // If the result dim is dynamic or scalar, we do not mind dynamic
+          // entries in the source.
+          if (resultType.getRank() == 0 || resultType.isDynamicDim(idx))
+            continue;
           for (auto targetDim : dims) {
             if (shape[targetDim] == ShapedType::kDynamic) shape[targetDim] = 1;
           }
@@ -1149,10 +1157,14 @@
                                                     newOperandType, operand);
         }
         // Generate collapse operation.
+        // For scalar collapses must pass an empty reassociation map.
+        if (resultType.getRank() == 0) reassociationMap->clear();
         rewriter.replaceOpWithNewOp<tensor::CollapseShapeOp>(
             reshapeOp, resultType, operand, *reassociationMap);
       } else {
         // Generate expand operation.
+          // For scalar expands must pass an empty reassociation map.
+        if (operandType.getRank() == 0) reassociationMap->clear();
         rewriter.replaceOpWithNewOp<tensor::ExpandShapeOp>(
             reshapeOp, resultType, operand, *reassociationMap);
       }
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgRandom.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgRandom.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgRandom.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgRandom.cpp
@@ -319,6 +319,7 @@
   auto reassociationIndices =
       getReassociationIndicesForCollapse(destTy.getShape(), srcTy.getShape());
   if (reassociationIndices.has_value()) {
+    if (srcTy.getRank() == 0) reassociationIndices->clear();
     src = builder.create<tensor::ExpandShapeOp>(loc, destTy, src,
                                                 reassociationIndices.value());
   }
@@ -328,6 +329,7 @@
   reassociationIndices =
       getReassociationIndicesForCollapse(srcTy.getShape(), destTy.getShape());
   if (reassociationIndices.has_value()) {
+    if (destTy.getRank() == 0) reassociationIndices->clear();
     src = builder.create<tensor::CollapseShapeOp>(loc, destTy, src,
                                                   reassociationIndices.value());
   }
diff --ruN a/stablehlo/stablehlo/dialect/VhloTypes.cpp b/stablehlo/stablehlo/dialect/VhloTypes.cpp
--- stablehlo/stablehlo/dialect/VhloTypes.cpp
+++ stablehlo/stablehlo/dialect/VhloTypes.cpp
@@ -16,6 +16,7 @@
 #include "stablehlo/dialect/VhloTypes.h"
 
 #include <cstdint>
+#include <optional>
 
 #include "llvm/ADT/SmallVectorExtras.h"
 #include "llvm/ADT/StringRef.h"
@@ -23,6 +24,7 @@
 #include "mlir/Dialect/Quant/IR/QuantTypes.h"
 #include "mlir/Dialect/Shape/IR/Shape.h"
 #include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/IR/MLIRContext.h"
 #include "mlir/IR/OpDefinition.h"
@@ -31,6 +33,7 @@
 #include "mlir/Support/LLVM.h"
 #include "mlir/Support/LogicalResult.h"
 #include "stablehlo/dialect/AssemblyFormat.h"
+#include "stablehlo/dialect/VhloTypes.h"
 
 namespace mlir {
 namespace vhlo {
@@ -397,6 +400,16 @@
   return success();
 }
 
+// RankedTensorV1Type implement ShapedTypeInterface
+mlir::ShapedType RankedTensorV1Type::cloneWith(
+  std::optional<llvm::ArrayRef<int64_t>> values, Type elementType) const {
+ArrayRef<int64_t> shape = values.value_or(getShape());
+return RankedTensorV1Type::get(getContext(), shape, elementType,
+                               getEncoding());
+}
+
+bool RankedTensorV1Type::hasRank() const { return true; }
+
 }  // namespace vhlo
 }  // namespace mlir
 
diff --ruN a/stablehlo/stablehlo/dialect/VhloTypes.td b/stablehlo/stablehlo/dialect/VhloTypes.td
--- stablehlo/stablehlo/dialect/VhloTypes.td
+++ stablehlo/stablehlo/dialect/VhloTypes.td
@@ -18,6 +18,7 @@
 #define STABLEHLO_DIALECT_VHLO_TYPES
 
 include "mlir/IR/AttrTypeBase.td"
+include "mlir/IR/BuiltinTypeInterfaces.td"
 include "stablehlo/dialect/VhloBase.td"
 include "stablehlo/dialect/VhloDialect.td"
 
@@ -33,8 +34,8 @@
   ];
 }
 
-class VHLO_TypeDef<string cppName, string name, string minVersion, string maxVersion>
-  : TypeDef<VHLO_Dialect, cppName, [VHLO_VersionedTypeInterface]> {
+class VHLO_TypeDef<string cppName, string name, string minVersion, string maxVersion, list<Trait> traits = []>
+  : TypeDef<VHLO_Dialect, cppName, [VHLO_VersionedTypeInterface] # traits> {
   let mnemonic = name;
   let extraClassDeclaration = [{
     mlir::vhlo::Version getMinVersion() {
@@ -186,7 +187,8 @@
 // At the moment, it is used to represent dimension bounds to support bounded
 // dynamism, and we're planning to look into it as part of the work on the
 // dynamism RFC.
-def VHLO_RankedTensorV1 : VHLO_TypeDef<"RankedTensorV1", "tensor_v1", "0.9.0", "current"> {
+def VHLO_RankedTensorV1 : VHLO_TypeDef<"RankedTensorV1", "tensor_v1", "0.9.0", "current",
+                                       [DeclareTypeInterfaceMethods<ShapedTypeInterface>]> {
   let parameters = (ins
     VHLO_Dims:$shape,
     "::mlir::Type":$elementType,

