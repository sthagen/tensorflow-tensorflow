diff --git a/docs/_toc.yaml b/docs/_toc.yaml
deleted file mode 100644
index 2757147..0000000
--- a/docs/_toc.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-toc:
-- heading: Shardy documentation
-- title: Getting started
-  section:
-  - title: Introduction
-    path: /shardy
-- title: Reference documentation
-  section:
-  - title: SDY dialect
-    path: /shardy/sdy_dialect
-  - title: SDY export passes
-    path: /shardy/sdy_export_passes
-  - title: SDY import passes
-    path: /shardy/sdy_import_passes
-  - title: SDY op interfaces
-    path: /shardy/sdy_op_interfaces
-  - title: SDY propagation passes
-    path: /shardy/sdy_propagation_passes
diff --git a/docs/sdy_export_passes.md b/docs/sdy_export_passes.md
index abfc31b..ce5d931 100755
--- a/docs/sdy_export_passes.md
+++ b/docs/sdy_export_passes.md
@@ -8,13 +8,13 @@ sharded operands and produce a sharded result without requiring any reshard
 communications (note that the operation might still require communication
 such as all-reduce or halo-swaps).
 
-After propagation, some operations may still have incompatible shardings.
+After propagation, some opeartions may still have incompatible shardings.
 
 Please note, when an axis (or sub-axis) is used to shard non-corresponding
 dimensions (e.g. non-contracting dimensions in matmul) across multiple
 tensors, or when an axis shards a dimension in one tensor but not the
 corresponding dimension in the other tensor, it is said that the operation
-has a sharding conflict. Hence, after this pass, the operations become
+has a sharding conflict. Hence, after this pass, the opeartions become
 conflict-free.
 
 This pass injects reshard operations explicitly so that, for each operation,
@@ -47,7 +47,7 @@ In the example above, there is a conflict since `lhs` and `rhs` tensors
 are both sharded on axis "x" on their non-contracting dimensions. Here,
 `rhs` tensor is resharded, before the dot operation, explicitly to be
 sharded only on its first dimension and on axis "x". This way, the dot
-operation becomes compatible.
+opearation becomes compatible.
 ### `-sdy-remove-sharding-groups`
 
 _Removes ShardingGroupOps after propagation._
diff --git a/shardy/dialect/sdy/transforms/export/passes.td b/shardy/dialect/sdy/transforms/export/passes.td
index 8c741b4..3c00db3 100644
--- a/shardy/dialect/sdy/transforms/export/passes.td
+++ b/shardy/dialect/sdy/transforms/export/passes.td
@@ -52,13 +52,13 @@ def InsertExplicitReshardsPass : Pass<"sdy-insert-explicit-reshards", "func::Fun
     communications (note that the operation might still require communication
     such as all-reduce or halo-swaps).
 
-    After propagation, some operations may still have incompatible shardings.
+    After propagation, some opeartions may still have incompatible shardings.
 
     Please note, when an axis (or sub-axis) is used to shard non-corresponding
     dimensions (e.g. non-contracting dimensions in matmul) across multiple
     tensors, or when an axis shards a dimension in one tensor but not the
     corresponding dimension in the other tensor, it is said that the operation
-    has a sharding conflict. Hence, after this pass, the operations become
+    has a sharding conflict. Hence, after this pass, the opeartions become
     conflict-free.
 
     This pass injects reshard operations explicitly so that, for each operation,
@@ -91,7 +91,7 @@ def InsertExplicitReshardsPass : Pass<"sdy-insert-explicit-reshards", "func::Fun
     are both sharded on axis "x" on their non-contracting dimensions. Here,
     `rhs` tensor is resharded, before the dot operation, explicitly to be
     sharded only on its first dimension and on axis "x". This way, the dot
-    operation becomes compatible.
+    opearation becomes compatible.
   }];
   let dependentDialects = ["mlir::sdy::SdyDialect"];
 }
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index d0f05d2..cf8280b 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "c6f3b7bcd0596d30f8dabecdfb9e44f9a07b6e4c"
-    LLVM_SHA256 = "13a1b39cfe51421fb1fd15828b76766d7680d854fc1039254b37f3f824ab47b7"
+    LLVM_COMMIT = "246b57cb2086b22ad8b41051c77e86ef478053a1"
+    LLVM_SHA256 = "2d46c080deb2cb6dbb9420a334be231040487bbfe87b5ed0b90bb69cac423500"
 
     tf_http_archive(
         name = name,
