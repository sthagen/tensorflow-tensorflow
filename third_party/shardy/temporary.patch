diff --git a/shardy/dialect/sdy/transforms/export/passes.td b/shardy/dialect/sdy/transforms/export/passes.td
index 58c9f74..64cfe7f 100644
--- a/shardy/dialect/sdy/transforms/export/passes.td
+++ b/shardy/dialect/sdy/transforms/export/passes.td
@@ -114,8 +114,8 @@ def TempExplicitReshardsForOptimizationsPass : Pass<"sdy-temp-explicit-reshards-
     This pass is a temporary solution until we can enable the
     `sdy-insert-explicit-reshards` pass by default.
 
-    It allows us to insert explicit reshards on specific operations for
-    optimizations.
+    It allows us to improve specific use cases where the partitioner does the
+    sub-optimal thing.
   }];
 }
 
diff --git a/shardy/dialect/sdy/transforms/export/temp_explicit_reshards_for_optimizations.cc b/shardy/dialect/sdy/transforms/export/temp_explicit_reshards_for_optimizations.cc
index b20b794..0642e3c 100644
--- a/shardy/dialect/sdy/transforms/export/temp_explicit_reshards_for_optimizations.cc
+++ b/shardy/dialect/sdy/transforms/export/temp_explicit_reshards_for_optimizations.cc
@@ -29,7 +29,6 @@ limitations under the License.
 #include "mlir/Support/LLVM.h"
 #include "shardy/dialect/sdy/ir/dialect.h"
 #include "shardy/dialect/sdy/ir/utils.h"
-#include "shardy/dialect/sdy/transforms/export/explicit_reshards_util.h"
 #include "shardy/dialect/sdy/transforms/export/passes.h"  // IWYU pragma: keep
 #include "shardy/dialect/sdy/transforms/propagation/op_sharding_rule_registry.h"
 #include "shardy/dialect/sdy/transforms/propagation/sharding_projection.h"
@@ -236,9 +235,6 @@ struct TempExplicitReshardsForOptimizationsPass
               [&](stablehlo::DotGeneralOp dotGeneralOp) {
                 processDot(dotGeneralOp, rewriter, symbolTable);
               });
-      if (op->getName().getStringRef().str() == "mhlo.ragged_dot") {
-        insertExplicitReshardsOnOp(op, rewriter, symbolTable);
-      }
     });
   }
 };
diff --git a/shardy/dialect/sdy/transforms/export/test/temp_explicit_reshards_for_optimizations.mlir b/shardy/dialect/sdy/transforms/export/test/temp_explicit_reshards_for_optimizations.mlir
index 48bcbcb..117954c 100644
--- a/shardy/dialect/sdy/transforms/export/test/temp_explicit_reshards_for_optimizations.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/temp_explicit_reshards_for_optimizations.mlir
@@ -1,8 +1,7 @@
-// RUN: sdy_opt %s -allow-unregistered-dialect  -sdy-temp-explicit-reshards-for-optimizations | FileCheck %s
+// RUN: sdy_opt %s -sdy-temp-explicit-reshards-for-optimizations | FileCheck %s
 
 sdy.mesh @mesh = <["x"=2, "y"=2, "z"=4]>
 sdy.mesh @other_mesh = <["x"=2, "y"=2]>
-sdy.mesh @mesh_abcd = <["a"=2, "b"=2, "c"=2, "d"=2]>
 
 // CHECK-LABEL: func @reshard_dot_result_to_match_lhs
 func.func @reshard_dot_result_to_match_lhs(
@@ -317,77 +316,3 @@ func.func @dot_result_conflicting_sharding_mismatch_with_reduction_axes_3(
       (tensor<4x2x32xf32>, tensor<2x32x8xf32>) -> tensor<4x8xf32>
   return %0 : tensor<4x8xf32>
 }
-
-// CHECK-LABEL: func @ragged_dot_mode_non_contracting
-func.func @ragged_dot_mode_non_contracting(
-    %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}, {"c"}]>},
-    %arg1: tensor<4x16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}, {"c"}, {"d"}]>},
-    %arg2: tensor<16x4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}]>}) -> tensor<16x32x8xf32> {
-  // CHECK: %[[RESHARD0:.*]] = sdy.reshard %arg0 <@mesh_abcd, [{"a"}, {}, {"c"}]> : tensor<16x32x64xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh_abcd, [{}, {"a"}, {"c"}, {"d"}]> : tensor<4x16x64x8xf32>
-  // CHECK: %[[RESHARD2:.*]] = sdy.reshard %arg2 <@mesh_abcd, [{"a"}, {}]> : tensor<16x4xi32>
-
-  // CHECK: %[[RAGGED_DOT:.*]] = "mhlo.ragged_dot"(%[[RESHARD0]], %[[RESHARD1]], %[[RESHARD2]]) <{
-  // CHECK: }>
-  // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{"a"}, {}, {"d"}]>]>
-
-  // CHECK: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"c"} %[[RAGGED_DOT]] out_sharding=<@mesh_abcd, [{"a"}, {}, {"d"}]> : tensor<16x32x8xf32>
-  // CHECK: %[[RESHARD3:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh_abcd, [{"a"}, {"b"}, {"c"}]> : tensor<16x32x8xf32>
-  // CHECK: return %[[RESHARD3]] : tensor<16x32x8xf32>
-  %0 = "mhlo.ragged_dot"(%arg0, %arg1, %arg2) <{ragged_dot_dimension_numbers =
-    #mhlo.ragged_dot<dot_dimension_numbers = #mhlo.dot<
-      lhs_batching_dimensions = [0], rhs_batching_dimensions = [1],
-      lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [2]>,
-      lhs_ragged_dimensions = [1], rhs_group_dimensions = [0]>}>
-    {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [m, i, l, k], [i, m])->([i, j, k]) {i=16, j=32, k=8, l=64, m=4} reduction={l} need_replication={j, m}>,
-     sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{"a"}, {"b"}, {"c"}]>]>}
-    : (tensor<16x32x64xf32>, tensor<4x16x64x8xf32>, tensor<16x4xi32>) -> tensor<16x32x8xf32>
-  return %0 : tensor<16x32x8xf32>
-}
-
-// CHECK-LABEL: func @ragged_dot_mode_contracting
-func.func @ragged_dot_mode_contracting(
-    %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}, {"c"}]>},
-    %arg1: tensor<16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}, {"c"}]>},
-    %arg2: tensor<16x4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}]>}) -> tensor<4x16x32x8xf32> {
-  // CHECK: %[[RESHARD0:.*]] = sdy.reshard %arg0 <@mesh_abcd, [{"a"}, {"b"}, {}]> : tensor<16x32x64xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh_abcd, [{"a"}, {}, {"d"}]> : tensor<16x64x8xf32>
-  // CHECK: %[[RESHARD2:.*]] = sdy.reshard %arg2 <@mesh_abcd, [{"a"}, {}]> : tensor<16x4xi32>
-
-  // CHECK: %[[RAGGED_DOT:.*]] = "mhlo.ragged_dot"(%[[RESHARD0]], %[[RESHARD1]], %[[RESHARD2]]) <{
-  // CHECK: }>
-  // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{}, {"a"}, {"b"}, {"d"}]>]>
-
-  // CHECK: %[[RESHARD3:.*]] = sdy.reshard %[[RAGGED_DOT]] <@mesh_abcd, [{"a"}, {"b"}, {"c"}, {"d"}]> : tensor<4x16x32x8xf32>
-  // CHECK: return %[[RESHARD3]] : tensor<4x16x32x8xf32>
-  %0 = "mhlo.ragged_dot"(%arg0, %arg1, %arg2) <{ragged_dot_dimension_numbers =
-    #mhlo.ragged_dot<dot_dimension_numbers = #mhlo.dot<
-    lhs_batching_dimensions = [0], rhs_batching_dimensions = [0],
-    lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>,
-    lhs_ragged_dimensions = [2]>}>
-    {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k], [i, m])->([m, i, j, k]) {i=16, j=32, k=8, l=64, m=4} need_replication={l, m}>,
-     sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{"a"}, {"b"}, {"c"}, {"d"}]>]>}
-    : (tensor<16x32x64xf32>, tensor<16x64x8xf32>, tensor<16x4xi32>) -> tensor<4x16x32x8xf32>
-  return %0 : tensor<4x16x32x8xf32>
-}
-
-// CHECK-LABEL: func @ragged_dot_mode_batch
-func.func @ragged_dot_mode_batch(
-    %arg0: tensor<16x32x64xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"b"}, {"c"}]>},
-    %arg1: tensor<16x64x8xf32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}, {"c"}, {"d"}]>},
-    %arg2: tensor<4xi32> {sdy.sharding=#sdy.sharding<@mesh_abcd, [{"a"}]>}) -> tensor<16x32x8xf32> {
-  // CHECK: %[[RAGGED_DOT:.*]] = "mhlo.ragged_dot"(%arg0, %arg1, %arg2) <{
-  // CHECK: }>
-  // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{"a"}, {"b"}, {"d"}]>]>
-  // CHECK: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"c"} %[[RAGGED_DOT]] out_sharding=<@mesh_abcd, [{"a"}, {"b"}, {"d"}]> : tensor<16x32x8xf32>
-  // CHECK: return %[[ALL_REDUCE]] : tensor<16x32x8xf32>
-  %0 = "mhlo.ragged_dot"(%arg0, %arg1, %arg2) <{ragged_dot_dimension_numbers =
-    #mhlo.ragged_dot<dot_dimension_numbers = #mhlo.dot<
-    lhs_batching_dimensions = [0], rhs_batching_dimensions = [0],
-    lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>,
-    lhs_ragged_dimensions = [0]>}>
-    {sdy.sharding_rule = #sdy.op_sharding_rule<([i, j, l], [i, l, k], [m])->([i, j, k]) {i=16, j=32, k=8, l=64, m=1} reduction={l}>,
-     sdy.sharding = #sdy.sharding_per_value<[<@mesh_abcd, [{"a"}, {"b"}, {"d"}]>]>}
-    : (tensor<16x32x64xf32>, tensor<16x64x8xf32>, tensor<4xi32>) -> tensor<16x32x8xf32>
-  return %0 : tensor<16x32x8xf32>
-}
diff --git a/shardy/dialect/sdy/transforms/import/sharding_group_import.cc b/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
index 6cfed8f..4061903 100644
--- a/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
+++ b/shardy/dialect/sdy/transforms/import/sharding_group_import.cc
@@ -117,8 +117,8 @@ GroupIdToShardingGroups unifyShardingGroups(
   int64_t reindexId = 0;
   SmallDenseMap<int64_t, int64_t> reindexMap;
   for (const auto& group : shardingGroupEquivalences) {
-    if (group.isLeader()) {
-      reindexMap[group.getData()] = reindexId++;
+    if (group->isLeader()) {
+      reindexMap[group->getData()] = reindexId++;
     }
   }
 
diff --git a/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir b/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
index 97099a1..6a711ae 100644
--- a/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
+++ b/shardy/dialect/sdy/transforms/import/test/sharding_group_import.mlir
@@ -64,8 +64,8 @@ func.func @sharding_groups_reindexes_ids(%arg0: tensor<4xf32>, %arg1: tensor<4xf
 
 // CHECK-LABEL: sharding_groups_reindex_ordering_matches_min_element_ordering
 func.func @sharding_groups_reindex_ordering_matches_min_element_ordering(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) {
-  // CHECK: sdy.sharding_group %arg0 group_id=1 : tensor<4xf32>
-  // CHECK: sdy.sharding_group %arg1 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg0 group_id=0 : tensor<4xf32>
+  // CHECK: sdy.sharding_group %arg1 group_id=1 : tensor<4xf32>
   // CHECK: sdy.sharding_group %arg2 group_id=2 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 567 : tensor<4xf32>
   sdy.sharding_group %arg0 group_id = 23 : tensor<4xf32>
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 2e6ff58..97282ec 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,23 +1,748 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaCXXScopeSpec.cpp b/clang/lib/Sema/SemaCXXScopeSpec.cpp
---- a/clang/lib/Sema/SemaCXXScopeSpec.cpp
-+++ b/clang/lib/Sema/SemaCXXScopeSpec.cpp
-@@ -873,6 +873,7 @@
-     DependentTemplateSpecializationTypeLoc SpecTL
-       = Builder.push<DependentTemplateSpecializationTypeLoc>(T);
-     SpecTL.setElaboratedKeywordLoc(SourceLocation());
-+    SpecTL.setQualifierLoc(NestedNameSpecifierLoc());
-     SpecTL.setTemplateKeywordLoc(TemplateKWLoc);
-     SpecTL.setTemplateNameLoc(TemplateNameLoc);
-     SpecTL.setLAngleLoc(LAngleLoc);
-diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/libc/BUILD.bazel b/utils/bazel/llvm-project-overlay/libc/BUILD.bazel
---- a/utils/bazel/llvm-project-overlay/libc/BUILD.bazel
-+++ b/utils/bazel/llvm-project-overlay/libc/BUILD.bazel
-@@ -1902,7 +1902,6 @@
-     name = "inv_trigf_utils",
-     srcs = ["src/math/generic/inv_trigf_utils.cpp"],
-     hdrs = [
--        "src/math/generic/atan_utils.h",
-         "src/math/generic/inv_trigf_utils.h",
-     ],
-     deps = [
+diff -ruN --strip-trailing-cr a/clang/lib/AST/ASTContext.cpp b/clang/lib/AST/ASTContext.cpp
+--- a/clang/lib/AST/ASTContext.cpp
++++ b/clang/lib/AST/ASTContext.cpp
+@@ -7011,7 +7011,7 @@
+         getCanonicalTemplateArgument(subst->getArgumentPack());
+     return getSubstTemplateTemplateParmPack(
+         canonArgPack, subst->getAssociatedDecl()->getCanonicalDecl(),
+-        subst->getFinal(), subst->getIndex());
++        subst->getIndex(), subst->getFinal());
+   }
+   case TemplateName::DeducedTemplate: {
+     assert(IgnoreDeduced == false);
+diff -ruN --strip-trailing-cr a/clang/lib/Sema/TreeTransform.h b/clang/lib/Sema/TreeTransform.h
+--- a/clang/lib/Sema/TreeTransform.h
++++ b/clang/lib/Sema/TreeTransform.h
+@@ -7765,17 +7765,23 @@
+   NewTemplateArgs.setLAngleLoc(TL.getLAngleLoc());
+   NewTemplateArgs.setRAngleLoc(TL.getRAngleLoc());
+ 
+-  typedef TemplateArgumentLocContainerIterator<
+-  DependentTemplateSpecializationTypeLoc> ArgIterator;
+-  if (getDerived().TransformTemplateArguments(ArgIterator(TL, 0),
+-                                              ArgIterator(TL, TL.getNumArgs()),
+-                                              NewTemplateArgs))
++  auto ArgsRange = llvm::make_range<TemplateArgumentLocContainerIterator<
++      DependentTemplateSpecializationTypeLoc>>({TL, 0}, {TL, TL.getNumArgs()});
++
++  if (getDerived().TransformTemplateArguments(ArgsRange.begin(),
++                                              ArgsRange.end(), NewTemplateArgs))
+     return QualType();
++  bool TemplateArgumentsChanged = !llvm::equal(
++      ArgsRange, NewTemplateArgs.arguments(),
++      [](const TemplateArgumentLoc &A, const TemplateArgumentLoc &B) {
++        return A.getArgument().structurallyEquals(B.getArgument());
++      });
+ 
+   const DependentTemplateStorage &DTN = T->getDependentTemplateName();
+ 
+   QualType Result = TL.getType();
+-  if (getDerived().AlwaysRebuild() || SS.getScopeRep() != DTN.getQualifier()) {
++  if (getDerived().AlwaysRebuild() || SS.getScopeRep() != DTN.getQualifier() ||
++      TemplateArgumentsChanged) {
+     TemplateName Name = getDerived().RebuildTemplateName(
+         SS, TL.getTemplateKeywordLoc(), DTN.getName(), TL.getTemplateNameLoc(),
+         /*ObjectType=*/QualType(), /*FirstQualifierInScope=*/nullptr,
+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderStmt.cpp b/clang/lib/Serialization/ASTReaderStmt.cpp
+--- a/clang/lib/Serialization/ASTReaderStmt.cpp
++++ b/clang/lib/Serialization/ASTReaderStmt.cpp
+@@ -2229,6 +2229,7 @@
+     E->PackIndex = Record.readInt();
+   else
+     E->PackIndex = 0;
++  E->Final = CurrentUnpackingBits->getNextBit();
+   E->SubstNonTypeTemplateParmExprBits.NameLoc = readSourceLocation();
+   E->Replacement = Record.readSubExpr();
+ }
+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTWriterStmt.cpp b/clang/lib/Serialization/ASTWriterStmt.cpp
+--- a/clang/lib/Serialization/ASTWriterStmt.cpp
++++ b/clang/lib/Serialization/ASTWriterStmt.cpp
+@@ -2229,6 +2229,7 @@
+   CurrentPackingBits.addBit((bool)E->getPackIndex());
+   if (auto PackIndex = E->getPackIndex())
+     Record.push_back(*PackIndex + 1);
++  CurrentPackingBits.addBit(E->getFinal());
+ 
+   Record.AddSourceLocation(E->getNameLoc());
+   Record.AddStmt(E->getReplacement());
+diff -ruN --strip-trailing-cr a/clang/test/CodeGen/include/cuda.h b/clang/test/CodeGen/include/cuda.h
+--- a/clang/test/CodeGen/include/cuda.h
++++ b/clang/test/CodeGen/include/cuda.h
+@@ -1,194 +0,0 @@
+-/* Minimal declarations for CUDA support.  Testing purposes only.
+- * This should stay in sync with clang/test/Headers/Inputs/include/cuda.h
+- */
+-#pragma once
+-
+-// Make this file work with nvcc, for testing compatibility.
+-
+-#ifndef __NVCC__
+-#define __constant__ __attribute__((constant))
+-#define __device__ __attribute__((device))
+-#define __global__ __attribute__((global))
+-#define __host__ __attribute__((host))
+-#define __shared__ __attribute__((shared))
+-#define __managed__ __attribute__((managed))
+-#define __launch_bounds__(...) __attribute__((launch_bounds(__VA_ARGS__)))
+-
+-struct dim3 {
+-  unsigned x, y, z;
+-  __host__ __device__ dim3(unsigned x, unsigned y = 1, unsigned z = 1) : x(x), y(y), z(z) {}
+-};
+-
+-// Host- and device-side placement new overloads.
+-void *operator new(__SIZE_TYPE__, void *p) { return p; }
+-void *operator new[](__SIZE_TYPE__, void *p) { return p; }
+-__device__ void *operator new(__SIZE_TYPE__, void *p) { return p; }
+-__device__ void *operator new[](__SIZE_TYPE__, void *p) { return p; }
+-
+-#define CUDA_VERSION 10100
+-
+-struct char1 {
+-  char x;
+-  __host__ __device__ char1(char x = 0) : x(x) {}
+-};
+-struct char2 {
+-  char x, y;
+-  __host__ __device__ char2(char x = 0, char y = 0) : x(x), y(y) {}
+-};
+-struct char4 {
+-  char x, y, z, w;
+-  __host__ __device__ char4(char x = 0, char y = 0, char z = 0, char w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct uchar1 {
+-  unsigned char x;
+-  __host__ __device__ uchar1(unsigned char x = 0) : x(x) {}
+-};
+-struct uchar2 {
+-  unsigned char x, y;
+-  __host__ __device__ uchar2(unsigned char x = 0, unsigned char y = 0) : x(x), y(y) {}
+-};
+-struct uchar4 {
+-  unsigned char x, y, z, w;
+-  __host__ __device__ uchar4(unsigned char x = 0, unsigned char y = 0, unsigned char z = 0, unsigned char w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct short1 {
+-  short x;
+-  __host__ __device__ short1(short x = 0) : x(x) {}
+-};
+-struct short2 {
+-  short x, y;
+-  __host__ __device__ short2(short x = 0, short y = 0) : x(x), y(y) {}
+-};
+-struct short4 {
+-  short x, y, z, w;
+-  __host__ __device__ short4(short x = 0, short y = 0, short z = 0, short w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct ushort1 {
+-  unsigned short x;
+-  __host__ __device__ ushort1(unsigned short x = 0) : x(x) {}
+-};
+-struct ushort2 {
+-  unsigned short x, y;
+-  __host__ __device__ ushort2(unsigned short x = 0, unsigned short y = 0) : x(x), y(y) {}
+-};
+-struct ushort4 {
+-  unsigned short x, y, z, w;
+-  __host__ __device__ ushort4(unsigned short x = 0, unsigned short y = 0, unsigned short z = 0, unsigned short w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct int1 {
+-  int x;
+-  __host__ __device__ int1(int x = 0) : x(x) {}
+-};
+-struct int2 {
+-  int x, y;
+-  __host__ __device__ int2(int x = 0, int y = 0) : x(x), y(y) {}
+-};
+-struct int4 {
+-  int x, y, z, w;
+-  __host__ __device__ int4(int x = 0, int y = 0, int z = 0, int w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct uint1 {
+-  unsigned x;
+-  __host__ __device__ uint1(unsigned x = 0) : x(x) {}
+-};
+-struct uint2 {
+-  unsigned x, y;
+-  __host__ __device__ uint2(unsigned x = 0, unsigned y = 0) : x(x), y(y) {}
+-};
+-struct uint3 {
+-  unsigned x, y, z;
+-  __host__ __device__ uint3(unsigned x = 0, unsigned y = 0, unsigned z = 0) : x(x), y(y), z(z) {}
+-};
+-struct uint4 {
+-  unsigned x, y, z, w;
+-  __host__ __device__ uint4(unsigned x = 0, unsigned y = 0, unsigned z = 0, unsigned w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct longlong1 {
+-  long long x;
+-  __host__ __device__ longlong1(long long x = 0) : x(x) {}
+-};
+-struct longlong2 {
+-  long long x, y;
+-  __host__ __device__ longlong2(long long x = 0, long long y = 0) : x(x), y(y) {}
+-};
+-struct longlong4 {
+-  long long x, y, z, w;
+-  __host__ __device__ longlong4(long long x = 0, long long y = 0, long long z = 0, long long w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct ulonglong1 {
+-  unsigned long long x;
+-  __host__ __device__ ulonglong1(unsigned long long x = 0) : x(x) {}
+-};
+-struct ulonglong2 {
+-  unsigned long long x, y;
+-  __host__ __device__ ulonglong2(unsigned long long x = 0, unsigned long long y = 0) : x(x), y(y) {}
+-};
+-struct ulonglong4 {
+-  unsigned long long x, y, z, w;
+-  __host__ __device__ ulonglong4(unsigned long long x = 0, unsigned long long y = 0, unsigned long long z = 0, unsigned long long w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct float1 {
+-  float x;
+-  __host__ __device__ float1(float x = 0) : x(x) {}
+-};
+-struct float2 {
+-  float x, y;
+-  __host__ __device__ float2(float x = 0, float y = 0) : x(x), y(y) {}
+-};
+-struct float4 {
+-  float x, y, z, w;
+-  __host__ __device__ float4(float x = 0, float y = 0, float z = 0, float w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-struct double1 {
+-  double x;
+-  __host__ __device__ double1(double x = 0) : x(x) {}
+-};
+-struct double2 {
+-  double x, y;
+-  __host__ __device__ double2(double x = 0, double y = 0) : x(x), y(y) {}
+-};
+-struct double4 {
+-  double x, y, z, w;
+-  __host__ __device__ double4(double x = 0, double y = 0, double z = 0, double w = 0) : x(x), y(y), z(z), w(w) {}
+-};
+-
+-typedef unsigned long long cudaTextureObject_t;
+-typedef unsigned long long cudaSurfaceObject_t;
+-
+-enum cudaTextureReadMode {
+-  cudaReadModeNormalizedFloat,
+-  cudaReadModeElementType
+-};
+-
+-enum cudaSurfaceBoundaryMode {
+-  cudaBoundaryModeZero,
+-  cudaBoundaryModeClamp,
+-  cudaBoundaryModeTrap
+-};
+-
+-enum {
+-  cudaTextureType1D,
+-  cudaTextureType2D,
+-  cudaTextureType3D,
+-  cudaTextureTypeCubemap,
+-  cudaTextureType1DLayered,
+-  cudaTextureType2DLayered,
+-  cudaTextureTypeCubemapLayered
+-};
+-
+-struct textureReference {};
+-template <class T, int texType = cudaTextureType1D,
+-          enum cudaTextureReadMode mode = cudaReadModeElementType>
+-struct __attribute__((device_builtin_texture_type)) texture
+-    : public textureReference {};
+-
+-#endif // !__NVCC__
+diff -ruN --strip-trailing-cr a/clang/test/CodeGen/Inputs/cuda.h b/clang/test/CodeGen/Inputs/cuda.h
+--- a/clang/test/CodeGen/Inputs/cuda.h
++++ b/clang/test/CodeGen/Inputs/cuda.h
+@@ -0,0 +1,194 @@
++/* Minimal declarations for CUDA support.  Testing purposes only.
++ * This should stay in sync with clang/test/Headers/Inputs/include/cuda.h
++ */
++#pragma once
++
++// Make this file work with nvcc, for testing compatibility.
++
++#ifndef __NVCC__
++#define __constant__ __attribute__((constant))
++#define __device__ __attribute__((device))
++#define __global__ __attribute__((global))
++#define __host__ __attribute__((host))
++#define __shared__ __attribute__((shared))
++#define __managed__ __attribute__((managed))
++#define __launch_bounds__(...) __attribute__((launch_bounds(__VA_ARGS__)))
++
++struct dim3 {
++  unsigned x, y, z;
++  __host__ __device__ dim3(unsigned x, unsigned y = 1, unsigned z = 1) : x(x), y(y), z(z) {}
++};
++
++// Host- and device-side placement new overloads.
++void *operator new(__SIZE_TYPE__, void *p) { return p; }
++void *operator new[](__SIZE_TYPE__, void *p) { return p; }
++__device__ void *operator new(__SIZE_TYPE__, void *p) { return p; }
++__device__ void *operator new[](__SIZE_TYPE__, void *p) { return p; }
++
++#define CUDA_VERSION 10100
++
++struct char1 {
++  char x;
++  __host__ __device__ char1(char x = 0) : x(x) {}
++};
++struct char2 {
++  char x, y;
++  __host__ __device__ char2(char x = 0, char y = 0) : x(x), y(y) {}
++};
++struct char4 {
++  char x, y, z, w;
++  __host__ __device__ char4(char x = 0, char y = 0, char z = 0, char w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct uchar1 {
++  unsigned char x;
++  __host__ __device__ uchar1(unsigned char x = 0) : x(x) {}
++};
++struct uchar2 {
++  unsigned char x, y;
++  __host__ __device__ uchar2(unsigned char x = 0, unsigned char y = 0) : x(x), y(y) {}
++};
++struct uchar4 {
++  unsigned char x, y, z, w;
++  __host__ __device__ uchar4(unsigned char x = 0, unsigned char y = 0, unsigned char z = 0, unsigned char w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct short1 {
++  short x;
++  __host__ __device__ short1(short x = 0) : x(x) {}
++};
++struct short2 {
++  short x, y;
++  __host__ __device__ short2(short x = 0, short y = 0) : x(x), y(y) {}
++};
++struct short4 {
++  short x, y, z, w;
++  __host__ __device__ short4(short x = 0, short y = 0, short z = 0, short w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct ushort1 {
++  unsigned short x;
++  __host__ __device__ ushort1(unsigned short x = 0) : x(x) {}
++};
++struct ushort2 {
++  unsigned short x, y;
++  __host__ __device__ ushort2(unsigned short x = 0, unsigned short y = 0) : x(x), y(y) {}
++};
++struct ushort4 {
++  unsigned short x, y, z, w;
++  __host__ __device__ ushort4(unsigned short x = 0, unsigned short y = 0, unsigned short z = 0, unsigned short w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct int1 {
++  int x;
++  __host__ __device__ int1(int x = 0) : x(x) {}
++};
++struct int2 {
++  int x, y;
++  __host__ __device__ int2(int x = 0, int y = 0) : x(x), y(y) {}
++};
++struct int4 {
++  int x, y, z, w;
++  __host__ __device__ int4(int x = 0, int y = 0, int z = 0, int w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct uint1 {
++  unsigned x;
++  __host__ __device__ uint1(unsigned x = 0) : x(x) {}
++};
++struct uint2 {
++  unsigned x, y;
++  __host__ __device__ uint2(unsigned x = 0, unsigned y = 0) : x(x), y(y) {}
++};
++struct uint3 {
++  unsigned x, y, z;
++  __host__ __device__ uint3(unsigned x = 0, unsigned y = 0, unsigned z = 0) : x(x), y(y), z(z) {}
++};
++struct uint4 {
++  unsigned x, y, z, w;
++  __host__ __device__ uint4(unsigned x = 0, unsigned y = 0, unsigned z = 0, unsigned w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct longlong1 {
++  long long x;
++  __host__ __device__ longlong1(long long x = 0) : x(x) {}
++};
++struct longlong2 {
++  long long x, y;
++  __host__ __device__ longlong2(long long x = 0, long long y = 0) : x(x), y(y) {}
++};
++struct longlong4 {
++  long long x, y, z, w;
++  __host__ __device__ longlong4(long long x = 0, long long y = 0, long long z = 0, long long w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct ulonglong1 {
++  unsigned long long x;
++  __host__ __device__ ulonglong1(unsigned long long x = 0) : x(x) {}
++};
++struct ulonglong2 {
++  unsigned long long x, y;
++  __host__ __device__ ulonglong2(unsigned long long x = 0, unsigned long long y = 0) : x(x), y(y) {}
++};
++struct ulonglong4 {
++  unsigned long long x, y, z, w;
++  __host__ __device__ ulonglong4(unsigned long long x = 0, unsigned long long y = 0, unsigned long long z = 0, unsigned long long w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct float1 {
++  float x;
++  __host__ __device__ float1(float x = 0) : x(x) {}
++};
++struct float2 {
++  float x, y;
++  __host__ __device__ float2(float x = 0, float y = 0) : x(x), y(y) {}
++};
++struct float4 {
++  float x, y, z, w;
++  __host__ __device__ float4(float x = 0, float y = 0, float z = 0, float w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++struct double1 {
++  double x;
++  __host__ __device__ double1(double x = 0) : x(x) {}
++};
++struct double2 {
++  double x, y;
++  __host__ __device__ double2(double x = 0, double y = 0) : x(x), y(y) {}
++};
++struct double4 {
++  double x, y, z, w;
++  __host__ __device__ double4(double x = 0, double y = 0, double z = 0, double w = 0) : x(x), y(y), z(z), w(w) {}
++};
++
++typedef unsigned long long cudaTextureObject_t;
++typedef unsigned long long cudaSurfaceObject_t;
++
++enum cudaTextureReadMode {
++  cudaReadModeNormalizedFloat,
++  cudaReadModeElementType
++};
++
++enum cudaSurfaceBoundaryMode {
++  cudaBoundaryModeZero,
++  cudaBoundaryModeClamp,
++  cudaBoundaryModeTrap
++};
++
++enum {
++  cudaTextureType1D,
++  cudaTextureType2D,
++  cudaTextureType3D,
++  cudaTextureTypeCubemap,
++  cudaTextureType1DLayered,
++  cudaTextureType2DLayered,
++  cudaTextureTypeCubemapLayered
++};
++
++struct textureReference {};
++template <class T, int texType = cudaTextureType1D,
++          enum cudaTextureReadMode mode = cudaReadModeElementType>
++struct __attribute__((device_builtin_texture_type)) texture
++    : public textureReference {};
++
++#endif // !__NVCC__
+diff -ruN --strip-trailing-cr a/clang/test/CodeGen/nvptx-surface.cu b/clang/test/CodeGen/nvptx-surface.cu
+--- a/clang/test/CodeGen/nvptx-surface.cu
++++ b/clang/test/CodeGen/nvptx-surface.cu
+@@ -1,6 +1,6 @@
+ // RUN: %clang_cc1 -triple nvptx-unknown-unknown -fcuda-is-device -O3 -o - %s -emit-llvm | FileCheck %s
+ // RUN: %clang_cc1 -triple nvptx64-unknown-unknown -fcuda-is-device -O3 -o - %s -emit-llvm | FileCheck %s
+-#include "include/cuda.h"
++#include "Inputs/cuda.h"
+ 
+ #include "__clang_cuda_texture_intrinsics.h"
+ 
+diff -ruN --strip-trailing-cr a/clang/test/SemaTemplate/dependent-names.cpp b/clang/test/SemaTemplate/dependent-names.cpp
+--- a/clang/test/SemaTemplate/dependent-names.cpp
++++ b/clang/test/SemaTemplate/dependent-names.cpp
+@@ -458,3 +458,12 @@
+   };
+   int f(b<a> ba) { return ba.add<0>(); }
+ }
++
++namespace TransformDependentTemplates {
++  template <class T> struct Test1 {
++    template <class T2>
++      using Arg = typename T::template Arg<T2>;
++    void f(Arg<void>);
++    void f(Arg<int>);
++  };
++} // namespace TransformDependentTemplates
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+--- a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
++++ b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+@@ -15391,12 +15391,20 @@
+ 
+   if (E->State == TreeEntry::SplitVectorize) {
+     Res = FindLastInst();
++    if (ArrayRef<TreeEntry *> Entries = getTreeEntries(Res); !Entries.empty()) {
++      for (auto *E : Entries) {
++        auto *I = dyn_cast_or_null<Instruction>(E->VectorizedValue);
++        if (!I)
++          I = &getLastInstructionInBundle(E);
++        if (Res->comesBefore(I))
++          Res = I;
++      }
++    }
+     return *Res;
+   }
+ 
+   // Set insertpoint for gathered loads to the very first load.
+-  if (E->State != TreeEntry::SplitVectorize &&
+-      GatheredLoadsEntriesFirst.has_value() &&
++  if (GatheredLoadsEntriesFirst.has_value() &&
+       E->Idx >= *GatheredLoadsEntriesFirst && !E->isGather() &&
+       E->getOpcode() == Instruction::Load) {
+     Res = FindFirstInst();
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp b/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
+--- a/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
++++ b/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
+@@ -2590,6 +2590,14 @@
+     if (R.mayWriteToMemory() && !InterleaveR)
+       return;
+ 
++    // Do not narrow interleave groups if there are VectorPointer recipes and
++    // the plan was unrolled. The recipe implicitly uses VF from
++    // VPTransformState.
++    // TODO: Remove restriction once the VF for the VectorPointer offset is
++    // modeled explicitly as operand.
++    if (isa<VPVectorPointerRecipe>(&R) && Plan.getUF() > 1)
++      return;
++
+     // All other ops are allowed, but we reject uses that cannot be converted
+     // when checking all allowed consumers (store interleave groups) below.
+     if (!InterleaveR)
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
+@@ -66,3 +66,91 @@
+ exit:
+   ret void
+ }
++
++define void @test_2xi64_with_wide_load(ptr noalias %data, ptr noalias %factor) {
++; CHECK-LABEL: define void @test_2xi64_with_wide_load(
++; CHECK-SAME: ptr noalias [[DATA:%.*]], ptr noalias [[FACTOR:%.*]]) {
++; CHECK-NEXT:  [[ENTRY:.*]]:
++; CHECK-NEXT:    br i1 false, label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
++; CHECK:       [[VECTOR_PH]]:
++; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
++; CHECK:       [[VECTOR_BODY]]:
++; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
++; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 2
++; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[INDEX]]
++; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i64, ptr [[TMP1]], i32 0
++; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds i64, ptr [[TMP1]], i32 2
++; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = load <2 x i64>, ptr [[TMP2]], align 8
++; CHECK-NEXT:    [[BROADCAST_SPLAT3:%.*]] = load <2 x i64>, ptr [[TMP3]], align 8
++; CHECK-NEXT:    [[TMP6:%.*]] = shl nsw i64 [[INDEX]], 1
++; CHECK-NEXT:    [[TMP7:%.*]] = shl nsw i64 [[TMP0]], 1
++; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP6]]
++; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP7]]
++; CHECK-NEXT:    [[WIDE_VEC:%.*]] = load <4 x i64>, ptr [[TMP8]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC2:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[WIDE_VEC3:%.*]] = load <4 x i64>, ptr [[TMP9]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = shufflevector <4 x i64> [[WIDE_VEC3]], <4 x i64> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC5:%.*]] = shufflevector <4 x i64> [[WIDE_VEC3]], <4 x i64> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[TMP10:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT]], [[WIDE_LOAD]]
++; CHECK-NEXT:    [[TMP11:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT3]], [[WIDE_LOAD1]]
++; CHECK-NEXT:    [[TMP15:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT]], [[STRIDED_VEC2]]
++; CHECK-NEXT:    [[TMP16:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT3]], [[STRIDED_VEC5]]
++; CHECK-NEXT:    [[TMP17:%.*]] = shufflevector <2 x i64> [[TMP10]], <2 x i64> [[TMP15]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x i64> [[TMP17]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x i64> [[INTERLEAVED_VEC]], ptr [[TMP8]], align 8
++; CHECK-NEXT:    [[TMP18:%.*]] = shufflevector <2 x i64> [[TMP11]], <2 x i64> [[TMP16]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC6:%.*]] = shufflevector <4 x i64> [[TMP18]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x i64> [[INTERLEAVED_VEC6]], ptr [[TMP9]], align 8
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
++; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], 100
++; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
++; CHECK:       [[MIDDLE_BLOCK]]:
++; CHECK-NEXT:    br i1 true, label %[[EXIT:.*]], label %[[SCALAR_PH]]
++; CHECK:       [[SCALAR_PH]]:
++; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 100, %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
++; CHECK-NEXT:    br label %[[LOOP:.*]]
++; CHECK:       [[LOOP]]:
++; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
++; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[IV]]
++; CHECK-NEXT:    [[L_FACTOR:%.*]] = load i64, ptr [[ARRAYIDX]], align 8
++; CHECK-NEXT:    [[TMP13:%.*]] = shl nsw i64 [[IV]], 1
++; CHECK-NEXT:    [[DATA_0:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP13]]
++; CHECK-NEXT:    [[L_0:%.*]] = load i64, ptr [[DATA_0]], align 8
++; CHECK-NEXT:    [[MUL_0:%.*]] = mul i64 [[L_FACTOR]], [[L_0]]
++; CHECK-NEXT:    store i64 [[MUL_0]], ptr [[DATA_0]], align 8
++; CHECK-NEXT:    [[TMP14:%.*]] = or disjoint i64 [[TMP13]], 1
++; CHECK-NEXT:    [[DATA_1:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP14]]
++; CHECK-NEXT:    [[L_1:%.*]] = load i64, ptr [[DATA_1]], align 8
++; CHECK-NEXT:    [[MUL_1:%.*]] = mul i64 [[L_FACTOR]], [[L_1]]
++; CHECK-NEXT:    store i64 [[MUL_1]], ptr [[DATA_1]], align 8
++; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV]], 1
++; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], 100
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
++; CHECK:       [[EXIT]]:
++; CHECK-NEXT:    ret void
++;
++entry:
++  br label %loop
++
++loop:
++  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]
++  %arrayidx = getelementptr inbounds i64, ptr %factor, i64 %iv
++  %l.factor = load i64, ptr %arrayidx, align 8
++  %1 = shl nsw i64 %iv, 1
++  %data.0 = getelementptr inbounds i64, ptr %data, i64 %1
++  %l.0 = load i64, ptr %data.0, align 8
++  %mul.0 = mul i64 %l.factor, %l.0
++  store i64 %mul.0, ptr %data.0, align 8
++  %3 = or disjoint i64 %1, 1
++  %data.1 = getelementptr inbounds i64, ptr %data, i64 %3
++  %l.1 = load i64, ptr %data.1, align 8
++  %mul.1 = mul i64 %l.factor, %l.1
++  store i64 %mul.1, ptr %data.1, align 8
++  %iv.next = add nuw nsw i64 %iv, 1
++  %ec = icmp eq i64 %iv.next, 100
++  br i1 %ec, label %exit, label %loop
++
++exit:
++  ret void
++}
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/split-node-last-inst-vectorized.ll b/llvm/test/Transforms/SLPVectorizer/X86/split-node-last-inst-vectorized.ll
+--- a/llvm/test/Transforms/SLPVectorizer/X86/split-node-last-inst-vectorized.ll
++++ b/llvm/test/Transforms/SLPVectorizer/X86/split-node-last-inst-vectorized.ll
+@@ -0,0 +1,99 @@
++; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
++; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-unknown-linux-gnu < %s | FileCheck %s
++
++define void @test(ptr %0, <8 x i8> %1) {
++; CHECK-LABEL: define void @test(
++; CHECK-SAME: ptr [[TMP0:%.*]], <8 x i8> [[TMP1:%.*]]) {
++; CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[TMP0]], align 2
++; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[TMP0]], i64 13436
++; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[TMP0]], i64 13536
++; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr i8, ptr [[TMP0]], i64 13437
++; CHECK-NEXT:    [[TMP7:%.*]] = load <8 x i8>, ptr [[TMP4]], align 4
++; CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <8 x i8> [[TMP1]], <8 x i8> poison, <8 x i32> <i32 0, i32 0, i32 2, i32 3, i32 4, i32 5, i32 0, i32 7>
++; CHECK-NEXT:    [[TMP9:%.*]] = insertelement <8 x i8> [[TMP7]], i8 [[TMP3]], i32 1
++; CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <8 x i8> [[TMP9]], <8 x i8> poison, <8 x i32> <i32 0, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
++; CHECK-NEXT:    [[TMP11:%.*]] = shufflevector <8 x i8> [[TMP8]], <8 x i8> poison, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 poison, i32 poison, i32 poison, i32 poison, i32 poison, i32 poison, i32 poison, i32 poison>
++; CHECK-NEXT:    [[TMP12:%.*]] = call <16 x i8> @llvm.vector.insert.v16i8.v8i8(<16 x i8> [[TMP11]], <8 x i8> [[TMP10]], i64 8)
++; CHECK-NEXT:    [[TMP13:%.*]] = load <8 x i8>, ptr [[TMP6]], align 1
++; CHECK-NEXT:    [[TMP14:%.*]] = shufflevector <8 x i8> [[TMP13]], <8 x i8> poison, <8 x i32> <i32 7, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6>
++; CHECK-NEXT:    [[TMP15:%.*]] = call <16 x i8> @llvm.vector.insert.v16i8.v8i8(<16 x i8> poison, <8 x i8> [[TMP7]], i64 0)
++; CHECK-NEXT:    [[TMP16:%.*]] = call <16 x i8> @llvm.vector.insert.v16i8.v8i8(<16 x i8> [[TMP15]], <8 x i8> [[TMP14]], i64 8)
++; CHECK-NEXT:    [[TMP17:%.*]] = or <16 x i8> [[TMP16]], [[TMP12]]
++; CHECK-NEXT:    store <16 x i8> [[TMP17]], ptr [[TMP5]], align 4
++; CHECK-NEXT:    ret void
++;
++  %3 = load i8, ptr %0, align 2
++  %4 = getelementptr i8, ptr %0, i64 13442
++  %5 = load i8, ptr %4, align 2
++  %6 = or i8 %5, %3
++  %7 = getelementptr i8, ptr %0, i64 13550
++  store i8 %6, ptr %7, align 2
++  %8 = extractelement <8 x i8> %1, i64 0
++  %9 = or i8 %5, %8
++  %10 = getelementptr i8, ptr %0, i64 13542
++  store i8 %9, ptr %10, align 2
++  %11 = getelementptr i8, ptr %0, i64 13438
++  %12 = load i8, ptr %11, align 2
++  %13 = or i8 %12, %3
++  %14 = getelementptr i8, ptr %0, i64 13546
++  store i8 %13, ptr %14, align 2
++  %15 = extractelement <8 x i8> %1, i64 2
++  %16 = or i8 %12, %15
++  %17 = getelementptr i8, ptr %0, i64 13538
++  store i8 %16, ptr %17, align 2
++  %18 = getelementptr i8, ptr %0, i64 13440
++  %19 = load i8, ptr %18, align 4
++  %20 = or i8 %19, %3
++  %21 = getelementptr i8, ptr %0, i64 13548
++  store i8 %20, ptr %21, align 4
++  %22 = extractelement <8 x i8> %1, i64 4
++  %23 = or i8 %19, %22
++  %24 = getelementptr i8, ptr %0, i64 13540
++  store i8 %23, ptr %24, align 4
++  %25 = getelementptr i8, ptr %0, i64 13436
++  %26 = load i8, ptr %25, align 4
++  %27 = getelementptr i8, ptr %0, i64 13444
++  %28 = load i8, ptr %27, align 4
++  %29 = or i8 %28, %26
++  %30 = getelementptr i8, ptr %0, i64 13544
++  store i8 %29, ptr %30, align 4
++  %31 = or i8 %26, %8
++  %32 = getelementptr i8, ptr %0, i64 13536
++  store i8 %31, ptr %32, align 4
++  %33 = getelementptr i8, ptr %0, i64 13443
++  %34 = load i8, ptr %33, align 1
++  %35 = or i8 %34, %3
++  %36 = getelementptr i8, ptr %0, i64 13551
++  store i8 %35, ptr %36, align 1
++  %37 = extractelement <8 x i8> %1, i64 7
++  %38 = or i8 %34, %37
++  %39 = getelementptr i8, ptr %0, i64 13543
++  store i8 %38, ptr %39, align 1
++  %40 = getelementptr i8, ptr %0, i64 13439
++  %41 = load i8, ptr %40, align 1
++  %42 = or i8 %41, %3
++  %43 = getelementptr i8, ptr %0, i64 13547
++  store i8 %42, ptr %43, align 1
++  %44 = extractelement <8 x i8> %1, i64 3
++  %45 = or i8 %41, %44
++  %46 = getelementptr i8, ptr %0, i64 13539
++  store i8 %45, ptr %46, align 1
++  %47 = getelementptr i8, ptr %0, i64 13441
++  %48 = load i8, ptr %47, align 1
++  %49 = or i8 %48, %3
++  %50 = getelementptr i8, ptr %0, i64 13549
++  store i8 %49, ptr %50, align 1
++  %51 = extractelement <8 x i8> %1, i64 5
++  %52 = or i8 %48, %51
++  %53 = getelementptr i8, ptr %0, i64 13541
++  store i8 %52, ptr %53, align 1
++  %54 = getelementptr i8, ptr %0, i64 13437
++  %55 = load i8, ptr %54, align 1
++  %56 = or i8 %55, %3
++  %57 = getelementptr i8, ptr %0, i64 13545
++  store i8 %56, ptr %57, align 1
++  %58 = or i8 %55, %8
++  %59 = getelementptr i8, ptr %0, i64 13537
++  store i8 %58, ptr %59, align 1
++  ret void
++}
+diff -ruN --strip-trailing-cr a/llvm/unittests/CodeGen/X86MCInstLowerTest.cpp b/llvm/unittests/CodeGen/X86MCInstLowerTest.cpp
+--- a/llvm/unittests/CodeGen/X86MCInstLowerTest.cpp
++++ b/llvm/unittests/CodeGen/X86MCInstLowerTest.cpp
+@@ -151,9 +151,10 @@
+   MachineModuleInfoWrapperPass *MMIWP =
+       new MachineModuleInfoWrapperPass(TM.get(), &*MCFoo);
+ 
+-  legacy::PassManager PassMgrF;
+   SmallString<1024> Buf;
+   llvm::raw_svector_ostream OS(Buf);
++  legacy::PassManager PassMgrF;
++
+   AsmPrinter *Printer =
+       addPassesToEmitFile(PassMgrF, OS, CodeGenFileType::AssemblyFile, MMIWP);
+   PassMgrF.run(*M);
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index 4a58099..c3bcd53 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "69f59d59cb02c06f1fac93ea5b19c2df9a684109"
-    LLVM_SHA256 = "2fd8dcec1da1c7166d58918d5f6330856edb37351248a5947661055313bb5d46"
+    LLVM_COMMIT = "cd54cb062bba9c90a8f3723bf66caa7effbcf259"
+    LLVM_SHA256 = "4054d0f174e80e9d0ca62af465a60252faabe4c7163612c0fdcb86898f7f266a"
 
     tf_http_archive(
         name = name,
